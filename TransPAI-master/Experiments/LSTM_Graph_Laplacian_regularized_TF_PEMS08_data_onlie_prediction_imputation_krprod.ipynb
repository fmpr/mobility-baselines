{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PeMS08 data prediction using LSTM Graph Laplacian Regularized Tensor Factorization\n",
    "\n",
    "> About the author: Jinming Yang (yangjm67@sjtu.edu.cn), Center for Intelligent Transportation Systems and Unmanned Aerial Systems Applications Research, School of Naval Architecture, Ocean and Civil Engineering, Shanghai Jiao Tong University, Shanghai 200240, China. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Organization: Tensor Structure\n",
    "\n",
    "We consider a high dimensional dataset collected from $M$ sensors for $T$ time steps. Each sensor collects $N$ different kinds of data. We express spatio-temporal dataset as a tensor $\\mathcal{Y}\\in\\mathbb{R}^{M\\times N\\times T}$. The three way tensor $\\mathcal{Y}$ can be factorized into three low rank feature matrices: spatial feature matrix $\\boldsymbol{U}\\in \\mathbb{R}^{M\\times r}$, data type feature matrix $\\boldsymbol{V}\\in \\mathbb{R}^{N\\times r}$ and temporal feature matrix $\\boldsymbol{X}\\in \\mathbb{R}^{T\\times r}$.\n",
    "\n",
    "\n",
    "## Long-short Term Memory Regularized Tensor Factorization(LSTM-ReTF)\n",
    "\n",
    "LSTM Regularized Tensor Factorization (LSTM-ReTF) is an approach to incorporate temporal dependencies into tensor CP factorization model which utilizes the well-studied Long-short term memory(LSTM) neural network to model temporal dependencies among temporal feature vectors ${\\boldsymbol{x}_t}$ explicitly. Let $f()$ stands for the feed forward process of the LSTM network temporal regularizer, then the temporal dependencies can be described as follows:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}'_t = f(\\boldsymbol{x}_{t-l_1},\\boldsymbol{x}_{t-l_2},...,\\boldsymbol{x}_{t-l_d})\n",
    "$$\n",
    "\n",
    "Where the lag set $\\mathcal{L}=\\left\\{l_1,l_2,...,l_d\\right\\}$ (e.g., $\\mathcal{L}=\\left\\{1,2,...,24\\right\\}$) indicates  the temporal correlation topology. We further define the LSTM temporal regularizer as follows:\n",
    "\n",
    "$$\\mathcal{R}_{t}\\left(X, var(f)\\right) = \\frac{1}{2}\\sum_{t=l_d}^n\\biggl(x_t - f(x_{t-l_1},x_{t-l_2},...,x_{t-l_d})\\biggr)^2$$\n",
    "\n",
    "where, $var(f)$ stands for the coefficient set in LSTM network, and $f()$ is the feed forward process of the LSTM network.\n",
    "\n",
    "Thus, LSTM-ReTF is given by solving\n",
    "$$\n",
    "\\min_{U,V,X,\\text{var}(f)}~~\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\sum_{r=1}^{R}u_{ir}v_{jr}x_{tr}\\right)^2\n",
    "+\\frac{\\lambda_{w}\\eta}{2}\\sum_{i=1}^{M}\\left\\|\\boldsymbol{w}_{i}\\right\\|_{2}^{2}+\\frac{\\lambda_{v}\\eta}{2}\\sum_{k=1}^{N}\\left\\|\\boldsymbol{v}_{k}\\right\\|_{2}^{2}+\\frac{\\lambda_{x}\\eta}{2}\\sum_{t=1}^{T}\\left\\|\\boldsymbol{x}_{t}\\right\\|_{2}^{2} \\\\\n",
    "+\\frac{\\lambda_{w}}{2}\\underbrace{\\sum_{i,j}E_{i,j}^w(\\boldsymbol{w}_i - \\boldsymbol{w}_j)^2}_{\\text{spatial regularizer}}+\\underbrace{\\frac{\\lambda_x}{2}\\sum_{t=l_d+1}^n\\biggl(\\boldsymbol{x}_t - \\boldsymbol{x}'_t\\biggr)^2}_{\\text{LSTM network temporal regularizer}}\n",
    "$$\n",
    "\n",
    "where $E_{i,j}^w$ is the edge weight of sensor $i$ and $j$ in the spatial topology graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the above minimization problem using alternative least square method(ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supper script denotes the iteration times.\n",
    "\n",
    "#### Updates for spatial feature vectors $\\boldsymbol{w}_i, i = 1, 2, ..., M$\n",
    "$$\n",
    "\\boldsymbol{w}_i^{(p+1)} = \\left(\\sum_{j,t:(i,j,t)\\in\\Omega}\\left(\\boldsymbol{v}_{j}^{(p)}\\odot\\boldsymbol{x}_{t}^{(p)}\\right)\\left(\\boldsymbol{v}_{j}^{(p)}\\odot\\boldsymbol{x}_{t}^{(p)}\\right)^{\\top}+\\lambda_{u}\\boldsymbol{I} + \\lambda_u\\sum_{k}E_{i,k}^u\\boldsymbol{I} \\right)^{-1} \\left(\\sum_{j,t:(i,j,t)\\in\\Omega}\\left(\\boldsymbol{v}_{j}^{(p)}\\odot\\boldsymbol{x}_{t}^{(p)}\\right)y_{ijt} + \\lambda_u\\sum_{k}E_{i,k}^u \\boldsymbol{u}_k^{(p)}\\right)\n",
    "$$\n",
    "\n",
    "#### Updates for data type feature vectors $\\boldsymbol{v}_k, k = 1, 2, ..., N$\n",
    "$$\n",
    "\\boldsymbol{v}_{k}^{(p+1)} = \\left(\\sum_{i,t:(i,k,t)\\in\\Omega}\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{x}_{t}^{(p)}\\right)\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{x}_{t}^{(p)}\\right)^{\\top}+\\lambda_{v}I\\right)^{-1}\\sum_{i,t:(i,k,t)\\in\\Omega}\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{x}_{t}^{(p)}\\right)y_{ikt}\n",
    "$$\n",
    "\n",
    "#### Updates for temporal feature vectors $\\boldsymbol{x}_t, t = 1, 2, ..., l_d$\n",
    "$$\n",
    "\\boldsymbol{x}_{t}^{(p+1)} = \\biggl(\\sum_{i,k:(i,k,t)\\in\\Omega}\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{v}_{k}^{(p+1)}\\right)\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{v}_{k}^{(p+1)}\\right)^{\\top}+\\lambda_x\\eta I \\biggr)^{-1} \\biggl(\\sum_{i,k:(i,k,t)\\in\\Omega}\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{v}_{k}^{(p+1)}\\right)y_{ikt}\\biggr)\n",
    "$$\n",
    "\n",
    "#### Updates for temporal vectors $\\boldsymbol{x}_t, t = l_d + 1, l_d + 2, ..., T$\n",
    "$$\n",
    "\\boldsymbol{x}_{t}^{(p+1)} = \\biggl(\\sum_{i,k:(i,k,t)\\in\\Omega}\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{v}_{k}^{(p+1)}\\right)\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{v}_{k}^{(p+1)}\\right)^{\\top} + \\lambda_x\\eta I+\\lambda_x I \\biggr)^{-1} \\biggl(\\sum_{i,k:(i,k,t)\\in\\Omega}\\left(\\boldsymbol{w}_{i}^{(p+1)}\\odot\\boldsymbol{v}_{k}^{(p+1)}\\right)y_{ikt} + \\lambda_x {{\\boldsymbol{x}'_t}^{(p+1)}} \\biggr)\n",
    "$$\n",
    "\n",
    "where $\\odot$ denotes element-wise product and ${{\\boldsymbol{x}'_t}^{(p+1)}} = f(\\boldsymbol{x}_{t-l_1}^{(p+1)},\\boldsymbol{x}_{t-l_2}^{(p+1)},...,\\boldsymbol{x}_{t-l_d}^{(p+1)}| \\theta^{(p)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from numpy.linalg import inv as inv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Computation Concepts\n",
    "\n",
    "### Kronecker product\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "Given two matrices $A\\in\\mathbb{R}^{m_1\\times n_1}$ and $B\\in\\mathbb{R}^{m_2\\times n_2}$, then, the **Kronecker product** between these two matrices is defined as\n",
    "\n",
    "$$A\\otimes B=\\left[ \\begin{array}{cccc} a_{11}B & a_{12}B & \\cdots & a_{1m_2}B \\\\ a_{21}B & a_{22}B & \\cdots & a_{2m_2}B \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m_11}B & a_{m_12}B & \\cdots & a_{m_1m_2}B \\\\ \\end{array} \\right]$$\n",
    "where the symbol $\\otimes$ denotes Kronecker product, and the size of resulted $A\\otimes B$ is $(m_1m_2)\\times (n_1n_2)$ (i.e., $m_1\\times m_2$ columns and $n_1\\times n_2$ rows).\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "If $A=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]$ and $B=\\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10 \\\\ \\end{array} \\right]$, then, we have\n",
    "\n",
    "$$A\\otimes B=\\left[ \\begin{array}{cc} 1\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] & 2\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] \\\\ 3\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] & 4\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cccccc} 5 & 6 & 7 & 10 & 12 & 14 \\\\ 8 & 9 & 10 & 16 & 18 & 20 \\\\ 15 & 18 & 21 & 20 & 24 & 28 \\\\ 24 & 27 & 30 & 32 & 36 & 40 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{4\\times 6}.$$\n",
    "\n",
    "### Khatri-Rao product (`kr_prod`)\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "Given two matrices $A=\\left( \\boldsymbol{a}_1,\\boldsymbol{a}_2,...,\\boldsymbol{a}_r \\right)\\in\\mathbb{R}^{m\\times r}$ and $B=\\left( \\boldsymbol{b}_1,\\boldsymbol{b}_2,...,\\boldsymbol{b}_r \\right)\\in\\mathbb{R}^{n\\times r}$ with same number of columns, then, the **Khatri-Rao product** (or **column-wise Kronecker product**) between $A$ and $B$ is given as follows,\n",
    "\n",
    "$$A\\odot B=\\left( \\boldsymbol{a}_1\\otimes \\boldsymbol{b}_1,\\boldsymbol{a}_2\\otimes \\boldsymbol{b}_2,...,\\boldsymbol{a}_r\\otimes \\boldsymbol{b}_r \\right)\\in\\mathbb{R}^{(mn)\\times r}$$\n",
    "where the symbol $\\odot$ denotes Khatri-Rao product, and $\\otimes$ denotes Kronecker product.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "If $A=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]=\\left( \\boldsymbol{a}_1,\\boldsymbol{a}_2 \\right) $ and $B=\\left[ \\begin{array}{cc} 5 & 6 \\\\ 7 & 8 \\\\ 9 & 10 \\\\ \\end{array} \\right]=\\left( \\boldsymbol{b}_1,\\boldsymbol{b}_2 \\right) $, then, we have\n",
    "\n",
    "$$A\\odot B=\\left( \\boldsymbol{a}_1\\otimes \\boldsymbol{b}_1,\\boldsymbol{a}_2\\otimes \\boldsymbol{b}_2 \\right) $$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cc} \\left[ \\begin{array}{c} 1 \\\\ 3 \\\\ \\end{array} \\right]\\otimes \\left[ \\begin{array}{c} 5 \\\\ 7 \\\\ 9 \\\\ \\end{array} \\right] & \\left[ \\begin{array}{c} 2 \\\\ 4 \\\\ \\end{array} \\right]\\otimes \\left[ \\begin{array}{c} 6 \\\\ 8 \\\\ 10 \\\\ \\end{array} \\right] \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cc} 5 & 12 \\\\ 7 & 16 \\\\ 9 & 20 \\\\ 15 & 24 \\\\ 21 & 32 \\\\ 27 & 40 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{6\\times 2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kr_prod(a, b):\n",
    "    return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 12]\n",
      " [ 7 16]\n",
      " [ 9 20]\n",
      " [15 24]\n",
      " [21 32]\n",
      " [27 40]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8], [9, 10]])\n",
    "print(kr_prod(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to combine feature matrices to data tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_combine(U, V, X):\n",
    "    return np.einsum('is, js, ts -> ijt', U, V, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to unfold tensor to matrix along specific dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the PeMSD8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing rate = PM 0.2\n"
     ]
    }
   ],
   "source": [
    "directory = '../Datasets/PEMS08/'\n",
    "\n",
    "missing_rate = 0.2\n",
    "mode = 'PM'\n",
    "\n",
    "Dist = np.load(directory + 'Dist.npy')\n",
    "A = np.load(directory + 'Adj.npy')\n",
    "dense_tensor = np.load( directory + 'PeMS08.npy')\n",
    "\n",
    "dim1, dim2, dim3 = dense_tensor.shape\n",
    "# dense_mat = dense_tensor.reshape(dim1 * dim2, dim3)\n",
    "# =============================================================================\n",
    "### Point-wise missing (PM) scenario\n",
    "### Set the PM scenario by:\n",
    "if mode == 'PM':\n",
    "    pm_random_mat = np.load(directory + 'pm_random_mat.npy')\n",
    "    binary_mat = np.round(pm_random_mat + 0.5 - missing_rate)\n",
    "    sparse_tensor = dense_tensor.copy()\n",
    "    for i in range(dim2):\n",
    "        sparse_tensor[:, i, :] = dense_tensor[:, i, :] * binary_mat\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "### Continuous-random missing (CM) scenario\n",
    "### Set the CM scenario by:\n",
    "if mode == 'CM':\n",
    "    cm_random_mat = np.load(directory + 'cm_random_mat.npy')\n",
    "    binary_mat = np.round(cm_random_mat + 0.5 - missing_rate)\n",
    "    dense_tensor_reshape = dense_tensor.reshape(dim1, dim2, 62, 288)\n",
    "    sparse_tensor_reshape = dense_tensor_reshape.copy()\n",
    "    for i in range(dim1):\n",
    "        for j in range(62):\n",
    "            sparse_tensor_reshape[i, :, j, :] = dense_tensor_reshape[i, :, j, :] * binary_mat[i, j]\n",
    "    sparse_tensor = sparse_tensor_reshape.reshape(dim1, dim2, dim3)\n",
    "# =============================================================================\n",
    "print('Missing rate = %s %.1f'%(mode, missing_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled inverse distance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217.6933921392941\n"
     ]
    }
   ],
   "source": [
    "def graph_weight_cal(dist, epsilon = 0.5):\n",
    "    dim = dist.shape[0]\n",
    "    distances = dist[np.nonzero(dist)].flatten()\n",
    "    std = distances.std()\n",
    "    print(std)\n",
    "    std_square = 5 * std ** 2\n",
    "    A = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if dist[i][j] > 0:\n",
    "                weight = np.exp(- dist[i][j] ** 2 / std_square)\n",
    "                if i != j and weight >= epsilon:\n",
    "                    A[i][j] = weight\n",
    "    return A\n",
    "A = graph_weight_cal(Dist, epsilon = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.74723109, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.74723109, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.9392094 ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.9392094 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setting\n",
    "This work mainly focuses on online spatial temporal data prediction and imputation, which is utilizing currently observed data(maybe incomplete) and history data(maybe incomplete) to make prediction of future data. In the mean time, the newly oberved incomplete data can be repaired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tensor = sparse_tensor[:, :, 8928:]\n",
    "dense_tensor = dense_tensor[:, :, 8928:]\n",
    "dim1, dim2, dim3 = sparse_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set is:\n",
      "(170, 3, 6048)\n",
      "\n",
      "The size of test set is:\n",
      "(170, 3, 2880)\n"
     ]
    }
   ],
   "source": [
    "test_len = 2880\n",
    "train_len = dim3 - test_len\n",
    "training_set = sparse_tensor[:, :, :train_len]\n",
    "test_set = sparse_tensor[:, :, train_len:]\n",
    "print('The size of training set is:')\n",
    "print(training_set.shape)\n",
    "print()\n",
    "print('The size of test set is:')\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving ground truth(real value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training set ground truth is:\n",
      "(170, 3, 6048)\n",
      "\n",
      "The size of test set ground truth is:\n",
      "(170, 3, 2880)\n"
     ]
    }
   ],
   "source": [
    "training_ground_truth = dense_tensor[:, :, :train_len]\n",
    "test_ground_truth = dense_tensor[:, :, train_len:]\n",
    "print('The size of training set ground truth is:')\n",
    "print(training_ground_truth.shape)\n",
    "print()\n",
    "print('The size of test set ground truth is:')\n",
    "print(test_ground_truth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM-GL-ReTF spatial temporal feature matrices and LSTM coefficients\n",
    "\n",
    "Bofore moving to the online prediction part of the framework, static data features(spatial feature matrix `W`, data type feature matrix `V` and temporal feature matrix `X`) and LSTM network coefficients(`var(f)`) should be trained first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to generate training samples for the LSTM neural network:\n",
    "\n",
    "- `dataset` is the spatial temporal matrix(training data matrix).\n",
    "- `rate` ranging from $(0, 1]$ stands for the sampling rate.\n",
    "- `time_lags` stands for the leg set which denotes the temporal correlation topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_samples(dataset, time_lags, rate):\n",
    "    dataX, dataY = [], []\n",
    "    data_len = dataset.shape[0] - np.max(time_lags)\n",
    "    t_sample = np.random.choice(data_len, int(rate * data_len), replace = False)\n",
    "    \n",
    "    for t in t_sample:\n",
    "        a = dataset[t + np.max(time_lags) - time_lags, :][::-1]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[t + np.max(time_lags), :])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates a LSTM network temporal regularizer. The input layer of the network has `rank` number of units, the LSTM layer has `rank` number of units and the full connection layer also has `rank` number of units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmmodel(rank, lag_len):\n",
    "    # create the LSTM network\n",
    "    model = Sequential()\n",
    "#     model.add(LSTM(rank, input_shape = (lag_len, rank), return_sequences = True)) # If you need multi-layer LSTM\n",
    "    model.add(LSTM(rank, input_shape = (lag_len, rank)))\n",
    "    model.add(Dense(rank))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error calculator\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "<li><b><code>mean_absolute_percentage_error</code>:</b> <font color=\"black\">Compute the value of Mean Absolute Percentage Error (MAPE).</font></li>\n",
    "<li><b><code>root_mean_squared_error</code>:</b> <font color=\"black\">Compute the value of Root Mean Square Error (RMSE).</font></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "> Note that $$\\mathrm{MAPE}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\left|y_{i}-\\hat{y}_{i}\\right|}{y_{i}} \\times 100, \\quad\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-{y}'_{i}\\right)^{2}},$$ where $n$ is the total number of estimated values, and $y_i$ and ${y}'_i$ are the actual value and its estimation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred, pos): \n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true[pos] - y_pred[pos]) / y_true[pos])) * 100\n",
    "def root_mean_squared_error(y_true, y_pred, pos): \n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(np.mean(np.square(y_true[pos] - y_pred[pos])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-ReTF training algorithm\n",
    "\n",
    "The function **LSTM_ReTF** is used to train spatial temporal feature matrices and LSTM network parameters.\n",
    "\n",
    "- `sparse_tensor` is the training set spatial temporal tensor.\n",
    "- `invD` is the pre-calculated scaled inverse distance matrix.\n",
    "- `init` is the initiated hyperparameters of LSTM-ReMF which includes the initiated spatial matrix `W` and the initiated temporal matrix `X`.\n",
    "- `time_lags` stands for the leg set which denotes the temporal correlation topology.\n",
    "- `lambda_w`, `lambda_x` and `eta` are regularizer parameters. \n",
    "- `sampling rate` is the ratio of data used to train the LSTM-full connection network.\n",
    "- `maxiter` is the maxiter time.\n",
    "- `track` is a 0 or 1 parameter that indicates whether to compute errors while training.\n",
    "- `patience` is the tolerance waiting step number. It is only required when `track` variable is 1.\n",
    "- `dense_tensor` is the training ground truth without data missing simulation. It is only required when `track` variable is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_GL_ReTF(sparse_tensor, invD, init, time_lags, lambda_w, lambda_g, lambda_v, lambda_x, eta, sampling_rate, maxiter, track, dense_tensor = 0):\n",
    "    W = init[\"W\"]\n",
    "    V = init[\"V\"]\n",
    "    X = init[\"X\"]\n",
    "    dim1, dim2, dim3 = sparse_tensor.shape\n",
    "    binary_tensor = np.zeros((dim1, dim2, dim3))\n",
    "    position = np.where((sparse_tensor != 0))\n",
    "    binary_tensor[position] = 1\n",
    "    d = len(time_lags)\n",
    "    max_lags = np.max(time_lags)\n",
    "    r = X.shape[1]\n",
    "    if track:\n",
    "        pos = np.where((sparse_tensor == 0) & (dense_tensor != 0))\n",
    "    model = lstmmodel(r, d)\n",
    "    model_reverse = lstmmodel(r, d)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for iters in range(maxiter):\n",
    "        var1 = kr_prod(X, V).T #(r, n*t)\n",
    "        var2 = kr_prod(var1, var1) # (r*r, n*t) \n",
    "        mat1 = ten2mat(binary_tensor, 0).T # (n*t, m)\n",
    "        mat2 = ten2mat(sparse_tensor, 0).T # (n*t, m)\n",
    "        for i in range(dim1):\n",
    "            vec1 = np.matmul(W.T, invD[i, :])\n",
    "            var_Lambda1 = np.matmul(var2, mat1[:, i]).reshape([rank, rank]) + lambda_w * np.eye(rank) + lambda_g * np.eye(rank) * np.sum(invD[i, :])\n",
    "            inv_var_Lambda1 = np.linalg.inv((var_Lambda1 + var_Lambda1.T)/2)\n",
    "            W[i, :] = np.matmul(inv_var_Lambda1, np.matmul(var1, mat2[:, i]) + lambda_g * vec1)\n",
    "\n",
    "        var1 = kr_prod(X, W).T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        mat1 = ten2mat(binary_tensor, 1).T # (m*t, n)\n",
    "        mat2 = ten2mat(sparse_tensor, 1).T # (m*t, n)\n",
    "        for j in range(dim2):\n",
    "            var_Lambda1 = np.matmul(var2, mat1[:, j]).reshape([rank, rank]) + lambda_v * eta * np.eye(rank)\n",
    "            inv_var_Lambda1 = np.linalg.inv((var_Lambda1 + var_Lambda1.T)/2)\n",
    "            V[j, :] = np.matmul(inv_var_Lambda1, np.matmul(var1, mat2[:, j]))\n",
    "        \n",
    "        var1 = kr_prod(V, W).T # (r, m*n)\n",
    "        var2 = kr_prod(var1, var1) # (r*r, m*n)\n",
    "        mat1 = ten2mat(binary_tensor, 2).T # (m*n, n)\n",
    "        mat2 = ten2mat(sparse_tensor, 2).T # (m*n, n)\n",
    "        for t in range(dim3):\n",
    "            if iters == 0 or t < max_lags:\n",
    "                var_Lambda1 = np.matmul(var2, mat1[:, t]).reshape([rank, rank]) + lambda_x * eta * np.eye(rank)\n",
    "                X[t, :] = np.matmul(inv((var_Lambda1 + var_Lambda1.T)/2), np.matmul(var1, mat2[:, t]))\n",
    "            else:\n",
    "                var_Lambda1 = np.matmul(var2, mat1[:, t]).reshape([rank, rank]) + lambda_x * np.eye(rank) + lambda_x * eta * np.eye(rank)\n",
    "                X_hat = X[t - time_lags, :][::-1]\n",
    "                X_hat_feed = X_hat[np.newaxis, :, :]\n",
    "                Qt =  model.predict(X_hat_feed)[0]\n",
    "                X[t, :] = np.matmul(inv((var_Lambda1 + var_Lambda1.T)/2),\n",
    "                                       (np.matmul(var1, mat2[:, t]) + lambda_x * Qt))\n",
    "\n",
    "        if iters == 0:\n",
    "            lstmX, lstmY = create_lstm_samples(X, time_lags, 1)\n",
    "            model.fit(lstmX, lstmY, epochs=20, batch_size=50, verbose=0)\n",
    "        else:\n",
    "            lstmX, lstmY = create_lstm_samples(X, time_lags, sampling_rate)\n",
    "            model.fit(lstmX, lstmY, epochs=2, batch_size=10, verbose=0)\n",
    "        if (iters + 1) % 10 == 0:\n",
    "            print('Iterations: %d, time cost: %ds'%((iters + 1), (time.time() - start_time)))\n",
    "            start_time = time.time()\n",
    "            if track:\n",
    "                tensor_hat = cp_combine(W, V, X)\n",
    "                tensor_hat[position] = sparse_tensor[position]\n",
    "                tensor_hat[tensor_hat < 0] = 0\n",
    "                rmse = root_mean_squared_error(dense_tensor, tensor_hat, pos)\n",
    "                mape = mean_absolute_percentage_error(dense_tensor, tensor_hat, pos)\n",
    "                print(np.mean(W))\n",
    "                print(np.mean(V))\n",
    "                print(np.mean(X))\n",
    "                print('Imputation RMSE = %.2f'%rmse)\n",
    "                print('Imputation MAPE = %.2f'%mape)\n",
    "            print()\n",
    "#     model.save('model_save\\lstm_trained1.h5')\n",
    "    tensor_hat = cp_combine(W, V, X)\n",
    "    tensor_hat[position] = sparse_tensor[position]\n",
    "    tensor_hat[tensor_hat < 0] = 0\n",
    "    return tensor_hat, W, V, X, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 3, 6048)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10, time cost: 153s\n",
      "9.22160435061816\n",
      "5.013191177216398\n",
      "-0.30011181931667424\n",
      "Imputation RMSE = 17.25\n",
      "Imputation MAPE = 22.64\n",
      "\n",
      "Iterations: 20, time cost: 135s\n",
      "8.028425640190587\n",
      "4.956336391614907\n",
      "-0.22263339333204182\n",
      "Imputation RMSE = 17.67\n",
      "Imputation MAPE = 22.12\n",
      "\n",
      "Iterations: 30, time cost: 137s\n",
      "7.232722486806132\n",
      "4.943137785924223\n",
      "-0.17490110004565443\n",
      "Imputation RMSE = 17.94\n",
      "Imputation MAPE = 21.83\n",
      "\n",
      "Iterations: 40, time cost: 133s\n",
      "6.532752259708474\n",
      "4.935415398521771\n",
      "-0.14243826217865463\n",
      "Imputation RMSE = 18.13\n",
      "Imputation MAPE = 21.63\n",
      "\n",
      "Iterations: 50, time cost: 133s\n",
      "5.909669403189389\n",
      "4.930665727301137\n",
      "-0.11550427206491597\n",
      "Imputation RMSE = 18.26\n",
      "Imputation MAPE = 21.46\n",
      "\n",
      "Iterations: 60, time cost: 143s\n",
      "5.358603390949798\n",
      "4.928666589760706\n",
      "-0.09318869167351733\n",
      "Imputation RMSE = 18.37\n",
      "Imputation MAPE = 21.31\n",
      "\n",
      "Iterations: 70, time cost: 144s\n",
      "4.869812211397462\n",
      "4.929795001203834\n",
      "-0.07400434998983998\n",
      "Imputation RMSE = 18.45\n",
      "Imputation MAPE = 21.23\n",
      "\n",
      "Iterations: 80, time cost: 145s\n",
      "4.436793879579541\n",
      "4.934153414042558\n",
      "-0.05803983403832337\n",
      "Imputation RMSE = 18.52\n",
      "Imputation MAPE = 21.17\n",
      "\n",
      "Iterations: 90, time cost: 145s\n",
      "4.043557557064923\n",
      "4.941600032980627\n",
      "-0.04355736870229572\n",
      "Imputation RMSE = 18.58\n",
      "Imputation MAPE = 21.16\n",
      "\n",
      "Iterations: 100, time cost: 145s\n",
      "3.6819706154422143\n",
      "4.9521665877209\n",
      "-0.030572600703704714\n",
      "Imputation RMSE = 18.63\n",
      "Imputation MAPE = 21.17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=10)\n",
    "rank = 20\n",
    "maxiter = 100\n",
    "eta = 0.1\n",
    "lambda_w = 100\n",
    "lambda_g = 100\n",
    "lambda_v = 100\n",
    "lambda_x = 100\n",
    "sampling_rate = 1.0\n",
    "time_lags = np.array([1, 2, 288])#np.arange(1, 25, 1)\n",
    "track = True\n",
    "dim1, dim2, dim3 = training_set.shape\n",
    "init = {\"W\": 0.1 * np.random.rand(dim1, rank), \"V\": 0.1 * np.random.rand(dim2, rank), \"X\": 0.1 * np.random.rand(dim3, rank)}\n",
    "tensor_hat, W, V, X, model = LSTM_GL_ReTF(training_set, A, init, time_lags, lambda_w, lambda_g, lambda_v,\n",
    "                                       lambda_x, eta, sampling_rate, maxiter, track, training_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online prediction and imputation with Online LSTM-ReTF\n",
    "In the context of spatiotemporal data online prediction and imputation, LSTM-ReTF takes current observation matrix $\\boldsymbol{Y}_{t}\\in\\mathbb{R}^{M\\times N}$ to update the previously predicted temporal feature vector $\\boldsymbol{x}'_t$ and impute possible missing entries in real time observation $\\boldsymbol{Y}_{t}$. Then, LSTM-ReTF use history temporal feature vectors $\\boldsymbol{x}_{t +1 - l}, l\\in \\mathcal{L}$ to make forecast of future temporal feature vecotr $x_{t+1}$. Finally, forecasted temporal feature vector $\\boldsymbol{x}_{t+1}$ will be multiplied by spatial embedding $\\boldsymbol{W}$ to calculate future data.\n",
    "\n",
    "Utilize pre-trained spatial feature matrix **W** of size [12, r], pre-trained data type feature matrix **V** of size [6, r], pre-trained LSTM coefficients **var(f)**, prestep temporal feature matrix **X0** of size [max(time_lags), r] and observations(may be incomplete) to make predictions of the next time step.\n",
    "\n",
    "### 1. Current temporal embedding $x_t$ calibration\n",
    "At time slot $t$, the newly observed data matrix $\\boldsymbol{Y}_t\\in\\mathbb{R}^{M\\times N}$ comes in, online LSTM-ReTF framework calibrates the previously predicted temporal feature vector $\\boldsymbol{x}'_t$. The calibrated temporal feature vector $x_t$ can be derived from the following optimization problem:\n",
    "$$\n",
    "\\min_{\\boldsymbol{x}_t}~~\\sum_{(i,j)\\in\\Omega_t}\\left(y_{ij,t}-\\sum_{r=1}^{R}u_{ir}v_{jr}x_{tr}\\right)^2+\\frac{\\lambda_{x}\\eta}{2}\\sum_{t=1}^{T}\\left\\|\\boldsymbol{x}_{t}\\right\\|_{2}^{2}+\\underbrace{\\frac{\\lambda_x}{2}\\sum_{t=l_d+1}^n\\biggl(\\boldsymbol{x}_t - \\boldsymbol{x}'_t\\biggr)^2}_{\\text{LSTM network regularizer}}\n",
    "$$\n",
    "where $\\Omega_{t}$ denotes the observed entries in the real time observation matrix $\\boldsymbol{Y}_{t}$.\n",
    "\n",
    "This optimizatoin problem can be solved by least square method, the uodating formulation for temporal embeddding $x_t$ can be derived as follows:\n",
    "$$\n",
    "\\boldsymbol{x}_{t} = \\biggl(\\sum_{(i,j)\\in\\Omega_{t}}(\\boldsymbol{w}_i\\circledast \\boldsymbol{v}_j)(\\boldsymbol{w}_i\\circledast \\boldsymbol{v}_j)^\\top + \\lambda_x I+\\lambda_x\\eta I \\biggr)^{-1}\\biggl(\\sum_{(i,j)\\in\\Omega_{t}}y_{ij,t}(\\boldsymbol{w}_i\\circledast \\boldsymbol{v}_j) + \\lambda_x \\boldsymbol{x}'_{t} \\biggr)\n",
    "$$\n",
    "\n",
    "### 2. Missing data imputation in current observation matrix $\\boldsymbol{Y}_{t}$\n",
    "With calibrated temporal feature vector $\\boldsymbol{x}_{t}$ and the pre-trained spatial feature matrix $\\boldsymbol{W}$, pre-trained data type feature matrix $\\boldsymbol{V}$, we can make imputation of the current observation by:\n",
    "$$\n",
    "{\\hat{\\boldsymbol{Y}_t}}_{i,j} = \\sum_{k=1}^r \\boldsymbol{w}_{ik}\\boldsymbol{v}_{jk}\\boldsymbol{x}'_{t, k}, i = 1, 2, ..., M, \\ j= 1, 2, ..., N.\n",
    "$$\n",
    "where $\\hat{\\boldsymbol{Y}_t}\\in\\mathbb{R}^{M\\times N}$ is the approximated observation matrix.\n",
    "\n",
    "### 3. Future temporal embedding $\\boldsymbol{x}_{t+1}$ prediction\n",
    "Making forecast of future temporal feature vector $\\boldsymbol{x}_{t+1}$.\n",
    "$$\n",
    "\\boldsymbol{x}'_{t + 1} = f(\\boldsymbol{x}_{t + 1 - l_1},\\boldsymbol{x}_{t + 1 - l_2},...,\\boldsymbol{x}_{t + 1 -l_d})\n",
    "$$\n",
    "\n",
    "### 4. Future data predction\n",
    "With dynamically calculated temporal feature vector $\\boldsymbol{x}_{t+1}$ and the pre-trained spatial feature matrix $\\boldsymbol{W}$, pre-trained data type feature matrix $\\boldsymbol{V}$, we can make prediction of the spatial temporal data of the next time slots as follows:\n",
    "$$\n",
    "{{\\boldsymbol{Y}'_t}^{(t+1)}}_{i,j} = \\sum_{k=1}^r \\boldsymbol{w}_{ik}\\boldsymbol{v}_{jk}\\boldsymbol{x+1}'_{t, k}, i = 1, 2, ..., M, \\ j= 1, 2, ..., N.\n",
    "$$\n",
    "where ${\\boldsymbol{Y}'_{t+1}}\\in\\mathbb{R}^{M\\times N}$ is the predicted observation matrix of the next time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online temporal embedding calibration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnlineLSTMReTF(sparse_mat, init, lambda_x, time_lags):\n",
    "    time_lags = time_lags[::-1]\n",
    "    W = init[\"W\"]\n",
    "    V = init[\"V\"]\n",
    "    X = init[\"X\"]\n",
    "    model = init[\"model\"]\n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    t, rank = X.shape\n",
    "    X_hat = X[t - 1 - time_lags, :].copy()\n",
    "    X_hat_feed = X_hat[np.newaxis, :, :]\n",
    "    Qt =  model.predict(X_hat_feed)[0]\n",
    "    \n",
    "    sparse_tensor = np.zeros((dim1, dim2, 1))\n",
    "    sparse_tensor[:, :, 0] = sparse_mat\n",
    "    position = np.where(sparse_tensor != 0)\n",
    "    binary_tensor = np.zeros(sparse_tensor.shape)\n",
    "    binary_tensor[position] = 1\n",
    "    var1 = kr_prod(V, W).T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var_mu = np.matmul(var1, ten2mat(sparse_tensor, 2).reshape([dim1 * dim2])) + lambda_x * Qt\n",
    "    inv_var_Lambda = inv(np.matmul(var2, ten2mat(binary_tensor, 2).reshape([dim1 * dim2])).reshape([rank, rank]) + lambda_x * np.eye(rank))\n",
    "    return np.matmul(inv_var_Lambda, var_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online prediction framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_prediction(sparse_tensor, init, time_lags, lambda_x, maxiter):\n",
    "    W = init[\"W\"]\n",
    "    V = init[\"V\"]\n",
    "    X = init[\"X\"]\n",
    "    model = init[\"model\"]\n",
    "    pre_step_num = X.shape[0]\n",
    "    rank = X.shape[1]\n",
    "    dim1, dim2, dim3 = sparse_tensor.shape\n",
    "    X_hat = np.zeros((dim3 + pre_step_num, rank))\n",
    "    tensor_pred = np.zeros((dim1, dim2, dim3))\n",
    "    X_hat[:pre_step_num,:] = X.copy()\n",
    "    start_time = time.time()\n",
    "    for t in range(dim3):\n",
    "        if t == 0:\n",
    "            X_star = X_hat[pre_step_num + t - time_lags, :][::-1]\n",
    "            X_star_feed = X_star[np.newaxis, :, :]\n",
    "            Qt =  model.predict(X_star_feed)[0]\n",
    "            X_hat[pre_step_num + t, :] = Qt.copy()\n",
    "        else:\n",
    "            sparse_mat = sparse_tensor[:, :, t - 1]\n",
    "            if np.where(sparse_mat > 0)[0].shape[0] > 0:\n",
    "                init = {\"W\": W, \"V\": V, \"X\": X_hat[pre_step_num + t - np.max(time_lags) - 1 : pre_step_num + t, :],\n",
    "                        \"model\": model}\n",
    "                X_c = OnlineLSTMReTF(sparse_mat, init, lambda_x/dim3, time_lags)\n",
    "                X_hat[pre_step_num + t - 1, :] = X_c.copy()\n",
    "                X_star = X_hat[pre_step_num + t - time_lags, :][::-1]\n",
    "                X_star_feed = X_star[np.newaxis, :, :]\n",
    "                Qt =  model.predict(X_star_feed)[0]\n",
    "                X_hat[pre_step_num + t, :] = Qt.copy()\n",
    "            else:\n",
    "                X_star = X_hat[pre_step_num + t - time_lags, :][::-1]\n",
    "                X_star_feed = X_star[np.newaxis, :, :]\n",
    "                Qt =  model.predict(X_star_feed)[0]\n",
    "                X_hat[pre_step_num + t, :] = Qt.copy()\n",
    "        tensor_pred[:, :, t] = np.einsum('ir, jr, r -> ij', W, V, X_hat[pre_step_num + t, :])\n",
    "        if (t + 1) % 1000 == 0:\n",
    "            print('Time step: %d, time cost: %d s'%((t + 1), (time.time() - start_time)))\n",
    "            start_time = time.time()\n",
    "            \n",
    "    sparse_mat = sparse_tensor[:, :, -1]\n",
    "    init = {\"W\": W, \"V\": V, \"X\": X_hat[dim2 + pre_step_num - np.max(time_lags) - 1 : , :], \"model\": model}\n",
    "    X_c = OnlineLSTMReTF(sparse_mat, init, lambda_x/dim2, time_lags)\n",
    "    X_hat[dim2 + pre_step_num - 1,:] = X_c.copy()\n",
    "    tensor_rec = cp_combine(W, V, X_hat[pre_step_num : , :])\n",
    "    return tensor_rec, tensor_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 1000, time cost: 3 s\n",
      "Time step: 2000, time cost: 3 s\n",
      "Shape of imputed data is:\n",
      "(170, 3, 2880)\n",
      "\n",
      "Shape of predicted data is:\n",
      "(170, 3, 2880)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "sc = 100000\n",
    "init = {\"W\": W, \"V\": V, \"X\": X[- np.max(time_lags): , :], \"model\": model}\n",
    "test_mat_rec, test_mat_pred = online_prediction(test_set, init, time_lags, sc * lambda_x, maxiter)\n",
    "print('Shape of imputed data is:')\n",
    "print(test_mat_rec.shape)\n",
    "print()\n",
    "print('Shape of predicted data is:')\n",
    "print(test_mat_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 first prediciton on test set:\n",
      "[131.70135841 132.21074799 131.32163445 133.13654547 130.9236932\n",
      " 128.13524087 124.30082013 118.62154246 111.75715259 112.41522463]\n",
      "\n",
      "10 first real value on test set\n",
      "[108. 107. 115. 110. 107. 108.  94.  92.  92.  93.]\n"
     ]
    }
   ],
   "source": [
    "print('10 first prediciton on test set:')\n",
    "print(test_mat_pred[0, 0, :10])\n",
    "print()\n",
    "print('10 first real value on test set')\n",
    "print(test_ground_truth[0, 0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the prediction and imputation error\n",
    "RMSE(root mean squared error) and MAPE(mean absolute percentage error) for both prediction and imputation on test set are calculated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction RMSE: 17.48 RMSE\n",
      "Test prediction MAPE: 20.73% MAPE\n",
      "\n",
      "Test prediction RMSE: 18.15 RMSE\n",
      "Test prediction MAPE: 21.34% MAPE\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "pos = np.where(test_ground_truth != 0)\n",
    "testPred_rmse = root_mean_squared_error(test_ground_truth, test_mat_pred, pos)\n",
    "print('Test prediction RMSE: %.2f RMSE' % (testPred_rmse))\n",
    "testPred_mape = mean_absolute_percentage_error(test_ground_truth, test_mat_pred, pos)\n",
    "print('Test prediction MAPE: %.2f%% MAPE' % (testPred_mape))\n",
    "print()\n",
    "\n",
    "# Imputation\n",
    "pos = np.where((test_set == 0) & (test_ground_truth != 0))\n",
    "testPred_rmse = root_mean_squared_error(test_ground_truth, test_mat_rec, pos)\n",
    "print('Test prediction RMSE: %.2f RMSE' % (testPred_rmse))\n",
    "testPred_mape = mean_absolute_percentage_error(test_ground_truth, test_mat_rec, pos)\n",
    "print('Test prediction MAPE: %.2f%% MAPE' % (testPred_mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
