{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation for LSTM Graph Laplacian Regularized Matrix Factorization\n",
    "\n",
    "> About the author: Jinming Yang (yangjm67@sjtu.edu.cn), Center for Intelligent Transportation Systems and Unmanned Aerial Systems Applications Research, School of Naval Architecture, Ocean and Civil Engineering, Shanghai Jiao Tong University, Shanghai 200240, China. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Seattle speed data set\n",
    "Guangzhou speed dataset is stored in a 323 by 17568 matrix.\n",
    "- **323** stands for 214 different road segments in Guangzhou.\n",
    "- **17568** stands for 17568 time slots over two months. (5 min a time slot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape:\n",
      "(323, 323)\n",
      "\n",
      "Dataset shape:\n",
      "(323, 17568)\n"
     ]
    }
   ],
   "source": [
    "# directory = '../datasets/Seattle_loop-data-set/'\n",
    "directory = 'E:/ACADEMIC/CODE/datasets/Seattle_loop-data-set/'\n",
    "ADJ = np.load( directory + 'Loop_Seattle_2015_A.npy')\n",
    "dense_mat = np.load( directory + 'dense_mat.npy')\n",
    "print('Adjacency matrix shape:')\n",
    "print(ADJ.shape)\n",
    "print()\n",
    "print('Dataset shape:')\n",
    "print(dense_mat.shape)\n",
    "\n",
    "missing_rate = 0.2\n",
    "# =============================================================================\n",
    "### Random missing (PM) scenario\n",
    "### Set the PM scenario by:\n",
    "rm_random_mat = np.load(directory + 'rm_random_mat.npy')\n",
    "binary_mat = np.round(rm_random_mat + 0.5 - missing_rate)\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "### Non-random missing (CM) scenario\n",
    "### Set the CM scenario by:\n",
    "# nm_random_mat = np.load(directory + 'nm_random_mat.npy')\n",
    "# binary_tensor = np.zeros((dense_mat.shape[0], 61, 288))\n",
    "# for i1 in range(binary_tensor.shape[0]):\n",
    "#     for i2 in range(binary_tensor.shape[1]):\n",
    "#         binary_tensor[i1, i2, :] = np.round(nm_random_mat[i1, i2] + 0.5 - missing_rate)\n",
    "# binary_mat = binary_tensor.reshape([binary_tensor.shape[0], binary_tensor.shape[1] * binary_tensor.shape[2]])\n",
    "# =============================================================================\n",
    "\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM-GL-ReMF spatial temporal feature matrices and LSTM coefficients\n",
    "\n",
    "Bofore moving to the online prediction part of the framework, static data features(spatial feature matrix `W` and temporal feature matrix `X`) and LSTM network coefficients(`var(f)`) should be trained first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to generate training samples for the LSTM neural network:\n",
    "\n",
    "- `dataset` is the spatial temporal matrix(training data matrix).\n",
    "- `rate` ranging from $(0, 1]$ stands for the sampling rate.\n",
    "- `time_lags` stands for the leg set which denotes the temporal correlation topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_samples(dataset, time_lags, rate):\n",
    "    dataX, dataY = [], []\n",
    "    data_len = dataset.shape[0] - np.max(time_lags)\n",
    "    t_sample = np.random.choice(data_len, int(rate * data_len), replace = False)\n",
    "    \n",
    "    for t in t_sample:\n",
    "        a = dataset[t + np.max(time_lags) - time_lags, :][::-1]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[t + np.max(time_lags), :])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates a LSTM-full connection neural network. The input layer of the network has `rank` number of units, the LSTM layer has `rank` number of units and the full connection layer also has `rank` number of units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmmodel(rank, lag_len):\n",
    "    # create the LSTM network\n",
    "    model = Sequential()\n",
    "#     model.add(LSTM(rank, input_shape = (lag_len, rank), return_sequences = True)) # If you need multi-layer LSTM\n",
    "    model.add(LSTM(rank, input_shape = (lag_len, rank)))\n",
    "    model.add(Dense(rank))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error calculator\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<ul>\n",
    "<li><b><code>mean_absolute_percentage_error</code>:</b> <font color=\"black\">Compute the value of Mean Absolute Percentage Error (MAPE).</font></li>\n",
    "<li><b><code>root_mean_squared_error</code>:</b> <font color=\"black\">Compute the value of Root Mean Square Error (RMSE).</font></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "> Note that $$\\mathrm{MAPE}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\left|y_{i}-\\hat{y}_{i}\\right|}{y_{i}} \\times 100, \\quad\\mathrm{RMSE}=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-{y}'_{i}\\right)^{2}},$$ where $n$ is the total number of estimated values, and $y_i$ and ${y}'_i$ are the actual value and its estimation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred, pos): \n",
    "    return np.mean(np.abs((y_true[pos] - y_pred[pos]) / y_true[pos])) * 100\n",
    "def root_mean_squared_error(y_true, y_pred, pos): \n",
    "    return np.sqrt(np.mean(np.square(y_true[pos] - y_pred[pos])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-GL-ReMF training algorithm\n",
    "\n",
    "The function **LSTM_GL_ReMF** is used to train spatial temporal feature matrices and LSTM network parameters.\n",
    "\n",
    "- `sparse_mat` is the training set spatial temporal matrix.\n",
    "- `ADJ` is the adjacency matrix of sensors.\n",
    "- `init` is the initiated hyperparameters of LSTM-ReMF which includes the initiated spatial matrix `W` and the initiated temporal matrix `X`.\n",
    "- `time_lags` stands for the leg set which denotes the temporal correlation topology.\n",
    "- `lambda_w`, `lambda_x` and `eta` are regularizer parameters. \n",
    "- `sampling rate` is the ratio of data used to train the LSTM-full connection network.\n",
    "- `maxiter` is the maxiter time.\n",
    "- `track` is a 0 or 1 parameter that indicates whether to compute errors while training.\n",
    "- `patience` is the tolerance waiting step number. It is only required when `track` variable is 1.\n",
    "- `dense_mat` is the training ground truth without data missing simulation. It is only required when `track` variable is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_GL_ReMF(sparse_mat, ADJ, init, time_lags, lambda_w, lambda_x, eta, sampling_rate, maxiter, track, patience = 5, dense_mat = 0):\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    binary_mat = np.zeros((dim1,dim2))\n",
    "    position = np.where((sparse_mat != 0))\n",
    "    binary_mat[position] = 1\n",
    "    d = len(time_lags)\n",
    "    max_lags = np.max(time_lags)\n",
    "    r = X.shape[1]\n",
    "    if track:\n",
    "        mape_pre = float(np.inf)\n",
    "        rmse_pre = float(np.inf)\n",
    "        pos_err = np.where((sparse_mat == 0) & (dense_mat != 0))\n",
    "        count = 0\n",
    "    model = lstmmodel(r, d)\n",
    "    model_reverse = lstmmodel(r, d)\n",
    "    start_time = time.time()\n",
    "    for iters in range(maxiter):\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            pos1 = np.where(ADJ[i,:] == 1)[0]\n",
    "            vec1 = np.sum(W[pos1, :], axis=0)\n",
    "            \n",
    "            Xt = X[pos0[0], :]\n",
    "            vec0 = np.matmul(Xt.T, sparse_mat[i, pos0[0]]) + vec1\n",
    "            mat0 = inv(np.matmul(Xt.T, Xt) + lambda_w * eta * np.eye(rank) + len(pos1) * lambda_w * np.eye(rank))\n",
    "            W[i, :] = np.matmul(mat0, vec0)\n",
    "\n",
    "        for t in range(dim2):\n",
    "            pos0 = np.where(sparse_mat[:, t] != 0)\n",
    "            Wt = W[pos0[0], :]\n",
    "            if iters == 0 or t < max_lags:\n",
    "                X[t, :] = np.matmul(inv(np.matmul(Wt.T, Wt) + lambda_x * eta * np.eye(r)), np.matmul(Wt.T, sparse_mat[pos0[0], t]))\n",
    "            else:\n",
    "                X_hat = X[t - time_lags, :][::-1]\n",
    "                X_hat_feed = X_hat[np.newaxis, :, :]\n",
    "                Qt =  model.predict(X_hat_feed)[0]\n",
    "                X[t, :] = np.matmul(inv(np.matmul(Wt.T, Wt)\n",
    "                                           + lambda_x * np.eye(r) + lambda_x * eta * np.eye(r)),\n",
    "                                       (np.matmul(Wt.T, sparse_mat[pos0[0], t]) + lambda_x * Qt))\n",
    "        \n",
    "        if iters == 0:\n",
    "            lstmX, lstmY = create_lstm_samples(X, time_lags, 1)\n",
    "            model.fit(lstmX, lstmY, epochs=20, batch_size=50, verbose=0)\n",
    "        else:\n",
    "            lstmX, lstmY = create_lstm_samples(X, time_lags, sampling_rate)\n",
    "            model.fit(lstmX, lstmY, epochs=2, batch_size=200, verbose=0)\n",
    "        if (iters + 1) % 10 == 0:\n",
    "#             print('Iterations: %d, time cost: %ds'%((iters + 1), (time.time() - start_time)))\n",
    "#             start_time = time.time()\n",
    "            if track:\n",
    "                mat_hat = np.matmul(W, X.T)\n",
    "                mat_hat[position] = sparse_mat[position]\n",
    "                mat_hat[mat_hat < 0] = 0\n",
    "                rmse = root_mean_squared_error(dense_mat, mat_hat, pos_err)\n",
    "                mape = mean_absolute_percentage_error(dense_mat, mat_hat, pos_err)\n",
    "#                 print('Imputation RMSE = %.2f'%rmse)\n",
    "#                 print('Imputation MAPE = %.2f'%mape)\n",
    "                rmse_dif = rmse_pre - rmse\n",
    "                mape_dif = mape_pre - mape\n",
    "                rmse_pre = rmse\n",
    "                mape_pre = mape\n",
    "                if rmse_dif < 0.001 and mape_dif < 0.001:\n",
    "                    count += 1\n",
    "                    if count == patience:\n",
    "#                         print('Wait step: %d'%count)\n",
    "                        break\n",
    "                else:\n",
    "                    count = 0\n",
    "            print()\n",
    "#     model.save('model_save\\lstm_trained1.h5')\n",
    "    mat_hat = np.matmul(W, X.T)\n",
    "    mat_hat[position] = sparse_mat[position]\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    return mat_hat, W, X, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online temporal embedding calibration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnlineLSTMReMF(sparse_vec, init, lambda_x, time_lags):\n",
    "    time_lags = time_lags[::-1]\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    lambda_x *= 6000\n",
    "    model = init[\"model\"]\n",
    "    dim = sparse_vec.shape[0]\n",
    "    t, rank = X.shape\n",
    "    X_hat = X[t - 1 - time_lags, :].copy()\n",
    "    X_hat_feed = X_hat[np.newaxis, :, :]\n",
    "    Qt =  model.predict(X_hat_feed)[0]\n",
    "    pos0 = np.where(sparse_vec != 0)\n",
    "    Wt = W[pos0[0], :]\n",
    "    var_mu = np.matmul(Wt.T, sparse_vec[pos0]) + lambda_x * Qt\n",
    "    inv_var_Lambda = inv(np.matmul(Wt.T, Wt) + lambda_x * np.eye(rank))\n",
    "    return np.matmul(inv_var_Lambda, var_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online prediction framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_prediction(sparse_mat, init, time_lags, lambda_x, eta):\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    model = init[\"model\"]\n",
    "    pre_step_num = X.shape[0]\n",
    "    rank = X.shape[1]\n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    X_hat = np.zeros((dim2 + pre_step_num, rank))\n",
    "    mat_pred = np.zeros((dim1, dim2))\n",
    "    X_hat[:pre_step_num,:] = X.copy()\n",
    "    start_time = time.time()\n",
    "    for t in range(dim2):\n",
    "        if t == 0:\n",
    "            X_star = X_hat[pre_step_num + t - time_lags, :][::-1]\n",
    "            X_star_feed = X_star[np.newaxis, :, :]\n",
    "            Qt =  model.predict(X_star_feed)[0]\n",
    "            X_hat[pre_step_num + t, :] = Qt.copy()\n",
    "        else:\n",
    "            sparse_vec = sparse_mat[:, t - 1]\n",
    "            if np.where(sparse_vec > 0)[0].shape[0] > 0:\n",
    "                init = {\"W\": W, \"X\": X_hat[pre_step_num + t - np.max(time_lags) - 1 : pre_step_num + t, :],\n",
    "                        \"model\": model}\n",
    "                X_c = OnlineLSTMReMF(sparse_vec, init, lambda_x/dim2, time_lags)\n",
    "                X_hat[pre_step_num + t - 1, :] = X_c.copy()\n",
    "                X_star = X_hat[pre_step_num + t - time_lags, :][::-1]\n",
    "                X_star_feed = X_star[np.newaxis, :, :]\n",
    "                Qt =  model.predict(X_star_feed)[0]\n",
    "                X_hat[pre_step_num + t, :] = Qt.copy()\n",
    "            else:\n",
    "                X_star = X_hat[pre_step_num + t - time_lags, :][::-1]\n",
    "                X_star_feed = X_star[np.newaxis, :, :]\n",
    "                Qt =  model.predict(X_star_feed)[0]\n",
    "                X_hat[pre_step_num + t, :] = Qt.copy()\n",
    "        mat_pred[:, t] = np.matmul(W, X_hat[pre_step_num + t, :])\n",
    "        if (t + 1) % 1000 == 0:\n",
    "            print('Time step: %d, time cost: %d s'%((t + 1), (time.time() - start_time)))\n",
    "            start_time = time.time()\n",
    "            \n",
    "    sparse_vec = sparse_mat[:, -1]\n",
    "    init = {\"W\": W, \"X\": X_hat[dim2 + pre_step_num - np.max(time_lags) - 1 : , :], \"model\": model}\n",
    "    X_c = OnlineLSTMReMF(sparse_vec, init, lambda_x/dim2, time_lags)\n",
    "    X_hat[dim2 + pre_step_num - 1,:] = X_c.copy()\n",
    "    mat_rec = np.matmul(W, X_hat[pre_step_num : , :].T)\n",
    "    return np.round(mat_rec), np.round(mat_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation for hyper-parmeter tuning\n",
    "Hyper-parameters are selected using grid search with sliding window cross validation. The original dataset is partitioned into three non-overlapping continuous sub-datasets, each containing 20 or 21 days' of the 61 days dataset. The hyper-parameter set with which the model obtains the minimum average RMSE on all sub-datasets are set to be the final hyper-parameters.\n",
    "\n",
    "\n",
    "### Hyper-parameters\n",
    "- `lambda_w = lamnda_x`\n",
    "- `eta`\n",
    "\n",
    "In the LSTM-GL-ReMF model, the parameters $\\lambda_w,\\lambda_x$ are used to weight the temporal and spatial regularizations and $\\eta$ is used to balance the temporal/spatial regularization and $l_2$ regularization. It is assumed that the temporal and spatial information of data is of same importance, so $\\lambda_w$ and $\\lambda_x$ are set to be equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning dataset using sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-dataset 1 of size (323, 5856) created.\n",
      "Sub-dataset 2 of size (323, 5856) created.\n",
      "Sub-dataset 3 of size (323, 5856) created.\n",
      "Data partition complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "test_len = 2 * 288\n",
    "dim1, dim2 = dense_mat.shape\n",
    "training_sets = []\n",
    "test_sets = []\n",
    "training_ground_truths = []\n",
    "test_ground_truths = []\n",
    "for i in range(N):\n",
    "    start = int(np.floor(dim2 / N) * i)\n",
    "    if i < N:\n",
    "        end = int(np.floor(dim2 / N) * (i + 1))\n",
    "    else:\n",
    "        end = dim2\n",
    "    span = end - start\n",
    "    training_sets.append(sparse_mat[:, start : end - test_len])\n",
    "    test_sets.append(sparse_mat[:, end - test_len : end])\n",
    "    training_ground_truths.append(dense_mat[:, start : end - test_len])\n",
    "    test_ground_truths.append(dense_mat[:, end - test_len : end])\n",
    "    print('Sub-dataset %d of size (%d, %d) created.'%(i+1, dim1, span))\n",
    "print('Data partition complete!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal lambda is 100, and the optimal eta is 0.2\n"
     ]
    }
   ],
   "source": [
    "rank = 60\n",
    "maxiter = 50\n",
    "sampling_rate = 1.0\n",
    "time_lags = np.array([1, 2, 288]) \n",
    "track = False\n",
    "patience = 10\n",
    "\n",
    "lambda_wxs = np.array([100, 200, 300, 400])\n",
    "etas = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "pred_RMSE_mat = np.zeros((len(lambda_wxs), len(etas)))\n",
    "for p in range(N):\n",
    "    for l_ind in range(len(lambda_wxs)):\n",
    "        for e_ind in range(len(etas)):\n",
    "            dim1, dim2 = training_sets[p].shape\n",
    "            init = {\"W\": 0.1 * np.random.rand(dim1, rank), \"X\": 0.1 * np.random.rand(dim2, rank)}\n",
    "            mat_hat, W, X, model = LSTM_GL_ReMF(training_sets[p], ADJ, init, time_lags, \n",
    "                               lambda_wxs[l_ind], lambda_wxs[l_ind], etas[e_ind], sampling_rate, maxiter, track, patience, training_ground_truths[p])\n",
    "            init = {\"W\": W, \"X\": X[- np.max(time_lags): , :], \"model\": model}\n",
    "            test_mat_rec, test_mat_pred = online_prediction(test_sets[p], init, time_lags, lambda_wxs[l_ind], etas[e_ind])\n",
    "            pos = np.where(test_ground_truths[p] != 0)\n",
    "            testPred_rmse = root_mean_squared_error(test_ground_truths[p], test_mat_pred, pos)\n",
    "            pred_RMSE_mat[l_ind, e_ind] += testPred_rmse\n",
    "# pred_RMSE_mat = pred_RMSE_mat / N\n",
    "pos = np.where(pred_RMSE_mat == np.min(pred_RMSE_mat))\n",
    "opt_lambda = lambda_wxs[pos[0][0]]\n",
    "opt_eta = etas[pos[1][0]]\n",
    "print('The optimal lambda is %d, and the optimal eta is %.1f'%(opt_lambda, opt_eta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
