{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rodr/code\n",
      "Namespace(batch_size=64, cheb_k=2, column_wise=False, comment='', cuda=True, dataset='PEMSD4', debug=False, default_graph=True, device=0, early_stop=True, early_stop_patience=15, embed_dim=10, epochs=100, f='/home/rodr/.local/share/jupyter/runtime/kernel-6038d5ec-3b1c-4b45-80bd-260bf458760e.json', g_type='agc', grad_norm=False, hid_dim=128, hid_hid_dim=128, horizon=12, input_dim=2, lag=12, log_dir='../runs', log_step=20, loss_func='mae', lr_decay=False, lr_decay_rate=0.3, lr_decay_step='5,20,40,70', lr_init=0.001, mae_thresh=None, mape_thresh=0.0, max_grad_norm=5, missing_rate=0.1, missing_test=False, mode='train', model='GCDE', model_path='', model_type='type1', normalizer='std', num_layers=3, num_nodes=307, output_dim=1, plot=False, real_value=True, seed=10, solver='rk4', teacher_forcing=False, tensorboard=False, test_ratio=0.2, tod=False, val_ratio=0.2, weight_decay=0.001)\n",
      "NeuralGCDE(\n",
      "  (func_f): FinalTanh_f(\n",
      "    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3\n",
      "    (linear_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear_out): Linear(in_features=128, out_features=256, bias=True)\n",
      "  )\n",
      "  (func_g): VectorField_g(\n",
      "    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3\n",
      "    (linear_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_out): Linear(in_features=128, out_features=16384, bias=True)\n",
      "  )\n",
      "  (end_conv): Conv2d(1, 12, kernel_size=(1, 128), stride=(1, 1))\n",
      "  (initial_h): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (initial_z): Linear(in_features=2, out_features=128, bias=True)\n",
      ")\n",
      "*****************Model Parameter*****************\n",
      "node_embeddings torch.Size([307, 10]) True\n",
      "func_f.linear_in.weight torch.Size([128, 128]) True\n",
      "func_f.linear_in.bias torch.Size([128]) True\n",
      "func_f.linears.0.weight torch.Size([128, 128]) True\n",
      "func_f.linears.0.bias torch.Size([128]) True\n",
      "func_f.linears.1.weight torch.Size([128, 128]) True\n",
      "func_f.linears.1.bias torch.Size([128]) True\n",
      "func_f.linear_out.weight torch.Size([256, 128]) True\n",
      "func_f.linear_out.bias torch.Size([256]) True\n",
      "func_g.node_embeddings torch.Size([307, 10]) True\n",
      "func_g.weights_pool torch.Size([10, 2, 128, 128]) True\n",
      "func_g.bias_pool torch.Size([10, 128]) True\n",
      "func_g.linear_in.weight torch.Size([128, 128]) True\n",
      "func_g.linear_in.bias torch.Size([128]) True\n",
      "func_g.linear_out.weight torch.Size([16384, 128]) True\n",
      "func_g.linear_out.bias torch.Size([16384]) True\n",
      "end_conv.weight torch.Size([12, 1, 1, 128]) True\n",
      "end_conv.bias torch.Size([12]) True\n",
      "initial_h.weight torch.Size([128, 2]) True\n",
      "initial_h.bias torch.Size([128]) True\n",
      "initial_z.weight torch.Size([128, 2]) True\n",
      "initial_z.bias torch.Size([128]) True\n",
      "Total params num: 2550024\n",
      "*****************Finish Parameter****************\n",
      "Load PEMSD4 Dataset shaped:  (16992, 307, 1) 919.0 0.0 211.7007794815878 180.0\n",
      "Normalize the dataset by Standard Normalization\n",
      "Train:  (10173, 12, 307, 1) (10173, 12, 307, 1)\n",
      "Val:  (3375, 12, 307, 1) (3375, 12, 307, 1)\n",
      "Test:  (3375, 12, 307, 1) (3375, 12, 307, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 17:23: Experiment log path in: ../runs/PEMSD4/02-10-17h23m_PEMSD4_GCDE_type1_embed{10}hid{128}hidhid{128}lyrs{3}lr{0.001}wd{0.001}\n",
      "2022-02-10 17:23: Argument batch_size: 64\n",
      "2022-02-10 17:23: Argument cheb_k: 2\n",
      "2022-02-10 17:23: Argument column_wise: False\n",
      "2022-02-10 17:23: Argument comment: ''\n",
      "2022-02-10 17:23: Argument cuda: True\n",
      "2022-02-10 17:23: Argument dataset: 'PEMSD4'\n",
      "2022-02-10 17:23: Argument debug: False\n",
      "2022-02-10 17:23: Argument default_graph: True\n",
      "2022-02-10 17:23: Argument device: 0\n",
      "2022-02-10 17:23: Argument early_stop: True\n",
      "2022-02-10 17:23: Argument early_stop_patience: 15\n",
      "2022-02-10 17:23: Argument embed_dim: 10\n",
      "2022-02-10 17:23: Argument epochs: 100\n",
      "2022-02-10 17:23: Argument f: '/home/rodr/.local/share/jupyter/runtime/kernel-6038d5ec-3b1c-4b45-80bd-260bf458760e.json'\n",
      "2022-02-10 17:23: Argument g_type: 'agc'\n",
      "2022-02-10 17:23: Argument grad_norm: False\n",
      "2022-02-10 17:23: Argument hid_dim: 128\n",
      "2022-02-10 17:23: Argument hid_hid_dim: 128\n",
      "2022-02-10 17:23: Argument horizon: 12\n",
      "2022-02-10 17:23: Argument input_dim: 2\n",
      "2022-02-10 17:23: Argument lag: 12\n",
      "2022-02-10 17:23: Argument log_dir: '../runs/PEMSD4/02-10-17h23m_PEMSD4_GCDE_type1_embed{10}hid{128}hidhid{128}lyrs{3}lr{0.001}wd{0.001}'\n",
      "2022-02-10 17:23: Argument log_step: 20\n",
      "2022-02-10 17:23: Argument loss_func: 'mae'\n",
      "2022-02-10 17:23: Argument lr_decay: False\n",
      "2022-02-10 17:23: Argument lr_decay_rate: 0.3\n",
      "2022-02-10 17:23: Argument lr_decay_step: '5,20,40,70'\n",
      "2022-02-10 17:23: Argument lr_init: 0.001\n",
      "2022-02-10 17:23: Argument mae_thresh: None\n",
      "2022-02-10 17:23: Argument mape_thresh: 0.0\n",
      "2022-02-10 17:23: Argument max_grad_norm: 5\n",
      "2022-02-10 17:23: Argument missing_rate: 0.1\n",
      "2022-02-10 17:23: Argument missing_test: False\n",
      "2022-02-10 17:23: Argument mode: 'train'\n",
      "2022-02-10 17:23: Argument model: 'GCDE'\n",
      "2022-02-10 17:23: Argument model_path: ''\n",
      "2022-02-10 17:23: Argument model_type: 'type1'\n",
      "2022-02-10 17:23: Argument normalizer: 'std'\n",
      "2022-02-10 17:23: Argument num_layers: 3\n",
      "2022-02-10 17:23: Argument num_nodes: 307\n",
      "2022-02-10 17:23: Argument output_dim: 1\n",
      "2022-02-10 17:23: Argument plot: False\n",
      "2022-02-10 17:23: Argument real_value: True\n",
      "2022-02-10 17:23: Argument seed: 10\n",
      "2022-02-10 17:23: Argument solver: 'rk4'\n",
      "2022-02-10 17:23: Argument teacher_forcing: False\n",
      "2022-02-10 17:23: Argument tensorboard: False\n",
      "2022-02-10 17:23: Argument test_ratio: 0.2\n",
      "2022-02-10 17:23: Argument tod: False\n",
      "2022-02-10 17:23: Argument val_ratio: 0.2\n",
      "2022-02-10 17:23: Argument weight_decay: 0.001\n",
      "2022-02-10 17:23: NeuralGCDE(\n",
      "  (func_f): FinalTanh_f(\n",
      "    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3\n",
      "    (linear_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear_out): Linear(in_features=128, out_features=256, bias=True)\n",
      "  )\n",
      "  (func_g): VectorField_g(\n",
      "    input_channels: 2, hidden_channels: 128, hidden_hidden_channels: 128, num_hidden_layers: 3\n",
      "    (linear_in): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear_out): Linear(in_features=128, out_features=16384, bias=True)\n",
      "  )\n",
      "  (end_conv): Conv2d(1, 12, kernel_size=(1, 128), stride=(1, 1))\n",
      "  (initial_h): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (initial_z): Linear(in_features=2, out_features=128, bias=True)\n",
      ")\n",
      "2022-02-10 17:23: Total params: 2550024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creat Log File in:  ../runs/PEMSD4/02-10-17h23m_PEMSD4_GCDE_type1_embed{10}hid{128}hidhid{128}lyrs{3}lr{0.001}wd{0.001}/run.log\n",
      "*****************Model Parameter*****************\n",
      "node_embeddings torch.Size([307, 10]) True\n",
      "func_f.linear_in.weight torch.Size([128, 128]) True\n",
      "func_f.linear_in.bias torch.Size([128]) True\n",
      "func_f.linears.0.weight torch.Size([128, 128]) True\n",
      "func_f.linears.0.bias torch.Size([128]) True\n",
      "func_f.linears.1.weight torch.Size([128, 128]) True\n",
      "func_f.linears.1.bias torch.Size([128]) True\n",
      "func_f.linear_out.weight torch.Size([256, 128]) True\n",
      "func_f.linear_out.bias torch.Size([256]) True\n",
      "func_g.node_embeddings torch.Size([307, 10]) True\n",
      "func_g.weights_pool torch.Size([10, 2, 128, 128]) True\n",
      "func_g.bias_pool torch.Size([10, 128]) True\n",
      "func_g.linear_in.weight torch.Size([128, 128]) True\n",
      "func_g.linear_in.bias torch.Size([128]) True\n",
      "func_g.linear_out.weight torch.Size([16384, 128]) True\n",
      "func_g.linear_out.bias torch.Size([16384]) True\n",
      "end_conv.weight torch.Size([12, 1, 1, 128]) True\n",
      "end_conv.bias torch.Size([12]) True\n",
      "initial_h.weight torch.Size([128, 2]) True\n",
      "initial_h.bias torch.Size([128]) True\n",
      "initial_z.weight torch.Size([128, 2]) True\n",
      "initial_z.bias torch.Size([128]) True\n",
      "Total params num: 2550024\n",
      "*****************Finish Parameter****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 17:23: Train Epoch 1: 0/158 Loss: 187.581009\n",
      "2022-02-10 17:24: Train Epoch 1: 20/158 Loss: 57.625172\n",
      "2022-02-10 17:25: Train Epoch 1: 40/158 Loss: 39.290855\n",
      "2022-02-10 17:26: Train Epoch 1: 60/158 Loss: 32.908375\n",
      "2022-02-10 17:28: Train Epoch 1: 80/158 Loss: 35.512909\n",
      "2022-02-10 17:29: Train Epoch 1: 100/158 Loss: 40.473694\n",
      "2022-02-10 17:30: Train Epoch 1: 120/158 Loss: 31.749498\n",
      "2022-02-10 17:31: Train Epoch 1: 140/158 Loss: 28.171183\n",
      "2022-02-10 17:32: **********Train Epoch 1: averaged Loss: 47.409154\n",
      "2022-02-10 17:33: **********Val Epoch 1: average Loss: 31.252103\n",
      "2022-02-10 17:33: *********************************Current best model saved!\n",
      "2022-02-10 17:33: Train Epoch 2: 0/158 Loss: 32.205299\n",
      "2022-02-10 17:35: Train Epoch 2: 20/158 Loss: 30.635521\n",
      "2022-02-10 17:36: Train Epoch 2: 40/158 Loss: 26.449610\n",
      "2022-02-10 17:37: Train Epoch 2: 60/158 Loss: 24.155886\n",
      "2022-02-10 17:38: Train Epoch 2: 80/158 Loss: 27.650103\n",
      "2022-02-10 17:40: Train Epoch 2: 100/158 Loss: 25.718758\n",
      "2022-02-10 17:41: Train Epoch 2: 120/158 Loss: 24.313553\n",
      "2022-02-10 17:42: Train Epoch 2: 140/158 Loss: 28.476295\n",
      "2022-02-10 17:43: **********Train Epoch 2: averaged Loss: 27.262288\n",
      "2022-02-10 17:44: **********Val Epoch 2: average Loss: 25.908880\n",
      "2022-02-10 17:44: *********************************Current best model saved!\n",
      "2022-02-10 17:44: Train Epoch 3: 0/158 Loss: 24.284636\n",
      "2022-02-10 17:46: Train Epoch 3: 20/158 Loss: 27.142687\n",
      "2022-02-10 17:47: Train Epoch 3: 40/158 Loss: 24.346300\n",
      "2022-02-10 17:48: Train Epoch 3: 60/158 Loss: 22.225473\n",
      "2022-02-10 17:49: Train Epoch 3: 80/158 Loss: 25.178274\n",
      "2022-02-10 17:51: Train Epoch 3: 100/158 Loss: 23.828785\n",
      "2022-02-10 17:52: Train Epoch 3: 120/158 Loss: 24.350574\n",
      "2022-02-10 17:53: Train Epoch 3: 140/158 Loss: 23.831120\n",
      "2022-02-10 17:55: **********Train Epoch 3: averaged Loss: 24.273771\n",
      "2022-02-10 17:56: **********Val Epoch 3: average Loss: 24.128439\n",
      "2022-02-10 17:56: *********************************Current best model saved!\n",
      "2022-02-10 17:56: Train Epoch 4: 0/158 Loss: 21.562433\n",
      "2022-02-10 17:57: Train Epoch 4: 20/158 Loss: 24.960323\n",
      "2022-02-10 17:58: Train Epoch 4: 40/158 Loss: 22.079008\n",
      "2022-02-10 17:59: Train Epoch 4: 60/158 Loss: 24.175957\n",
      "2022-02-10 18:01: Train Epoch 4: 80/158 Loss: 23.508856\n",
      "2022-02-10 18:02: Train Epoch 4: 100/158 Loss: 22.629520\n",
      "2022-02-10 18:03: Train Epoch 4: 120/158 Loss: 22.360313\n",
      "2022-02-10 18:05: Train Epoch 4: 140/158 Loss: 22.305616\n",
      "2022-02-10 18:06: **********Train Epoch 4: averaged Loss: 23.427271\n",
      "2022-02-10 18:07: **********Val Epoch 4: average Loss: 23.332839\n",
      "2022-02-10 18:07: *********************************Current best model saved!\n",
      "2022-02-10 18:07: Train Epoch 5: 0/158 Loss: 21.886978\n",
      "2022-02-10 18:08: Train Epoch 5: 20/158 Loss: 23.599163\n",
      "2022-02-10 18:09: Train Epoch 5: 40/158 Loss: 23.553310\n",
      "2022-02-10 18:11: Train Epoch 5: 60/158 Loss: 24.337633\n",
      "2022-02-10 18:12: Train Epoch 5: 80/158 Loss: 23.106459\n",
      "2022-02-10 18:13: Train Epoch 5: 100/158 Loss: 24.226559\n",
      "2022-02-10 18:14: Train Epoch 5: 120/158 Loss: 21.815092\n",
      "2022-02-10 18:16: Train Epoch 5: 140/158 Loss: 22.574389\n",
      "2022-02-10 18:17: **********Train Epoch 5: averaged Loss: 22.440917\n",
      "2022-02-10 18:18: **********Val Epoch 5: average Loss: 22.702147\n",
      "2022-02-10 18:18: *********************************Current best model saved!\n",
      "2022-02-10 18:18: Train Epoch 6: 0/158 Loss: 24.503239\n",
      "2022-02-10 18:19: Train Epoch 6: 20/158 Loss: 21.213470\n",
      "2022-02-10 18:20: Train Epoch 6: 40/158 Loss: 21.532753\n",
      "2022-02-10 18:22: Train Epoch 6: 60/158 Loss: 22.709364\n",
      "2022-02-10 18:23: Train Epoch 6: 80/158 Loss: 22.353191\n",
      "2022-02-10 18:24: Train Epoch 6: 100/158 Loss: 22.138174\n",
      "2022-02-10 18:25: Train Epoch 6: 120/158 Loss: 21.559748\n",
      "2022-02-10 18:27: Train Epoch 6: 140/158 Loss: 21.175629\n",
      "2022-02-10 18:28: **********Train Epoch 6: averaged Loss: 21.722965\n",
      "2022-02-10 18:29: **********Val Epoch 6: average Loss: 22.991594\n",
      "2022-02-10 18:29: Train Epoch 7: 0/158 Loss: 22.481775\n",
      "2022-02-10 18:30: Train Epoch 7: 20/158 Loss: 19.859051\n",
      "2022-02-10 18:32: Train Epoch 7: 40/158 Loss: 21.098335\n",
      "2022-02-10 18:33: Train Epoch 7: 60/158 Loss: 21.894651\n",
      "2022-02-10 18:35: Train Epoch 7: 80/158 Loss: 21.149296\n",
      "2022-02-10 18:36: Train Epoch 7: 100/158 Loss: 22.826727\n",
      "2022-02-10 18:38: Train Epoch 7: 120/158 Loss: 20.587290\n",
      "2022-02-10 18:39: Train Epoch 7: 140/158 Loss: 21.422651\n",
      "2022-02-10 18:40: **********Train Epoch 7: averaged Loss: 21.239593\n",
      "2022-02-10 18:41: **********Val Epoch 7: average Loss: 21.808260\n",
      "2022-02-10 18:41: *********************************Current best model saved!\n",
      "2022-02-10 18:41: Train Epoch 8: 0/158 Loss: 20.985241\n",
      "2022-02-10 18:43: Train Epoch 8: 20/158 Loss: 21.371862\n",
      "2022-02-10 18:44: Train Epoch 8: 40/158 Loss: 21.305317\n",
      "2022-02-10 18:46: Train Epoch 8: 60/158 Loss: 20.873566\n",
      "2022-02-10 18:47: Train Epoch 8: 80/158 Loss: 22.186338\n",
      "2022-02-10 18:49: Train Epoch 8: 100/158 Loss: 21.659189\n",
      "2022-02-10 18:50: Train Epoch 8: 120/158 Loss: 20.551939\n",
      "2022-02-10 18:52: Train Epoch 8: 140/158 Loss: 21.522989\n",
      "2022-02-10 18:53: **********Train Epoch 8: averaged Loss: 21.098648\n",
      "2022-02-10 18:54: **********Val Epoch 8: average Loss: 23.195443\n",
      "2022-02-10 18:54: Train Epoch 9: 0/158 Loss: 21.368643\n",
      "2022-02-10 18:55: Train Epoch 9: 20/158 Loss: 19.841341\n",
      "2022-02-10 18:56: Train Epoch 9: 40/158 Loss: 19.499006\n",
      "2022-02-10 18:58: Train Epoch 9: 60/158 Loss: 21.681950\n",
      "2022-02-10 18:59: Train Epoch 9: 80/158 Loss: 20.430056\n",
      "2022-02-10 19:00: Train Epoch 9: 100/158 Loss: 20.605173\n",
      "2022-02-10 19:02: Train Epoch 9: 120/158 Loss: 20.176718\n",
      "2022-02-10 19:03: Train Epoch 9: 140/158 Loss: 20.243145\n",
      "2022-02-10 19:04: **********Train Epoch 9: averaged Loss: 20.729662\n",
      "2022-02-10 19:05: **********Val Epoch 9: average Loss: 21.781866\n",
      "2022-02-10 19:05: *********************************Current best model saved!\n",
      "2022-02-10 19:05: Train Epoch 10: 0/158 Loss: 21.531311\n",
      "2022-02-10 19:06: Train Epoch 10: 20/158 Loss: 21.394602\n",
      "2022-02-10 19:07: Train Epoch 10: 40/158 Loss: 20.521843\n",
      "2022-02-10 19:09: Train Epoch 10: 60/158 Loss: 20.487078\n",
      "2022-02-10 19:10: Train Epoch 10: 80/158 Loss: 19.150047\n",
      "2022-02-10 19:11: Train Epoch 10: 100/158 Loss: 19.244711\n",
      "2022-02-10 19:13: Train Epoch 10: 120/158 Loss: 21.595146\n",
      "2022-02-10 19:14: Train Epoch 10: 140/158 Loss: 20.220675\n",
      "2022-02-10 19:15: **********Train Epoch 10: averaged Loss: 20.473715\n",
      "2022-02-10 19:16: **********Val Epoch 10: average Loss: 21.454109\n",
      "2022-02-10 19:16: *********************************Current best model saved!\n",
      "2022-02-10 19:16: Train Epoch 11: 0/158 Loss: 19.964701\n",
      "2022-02-10 19:17: Train Epoch 11: 20/158 Loss: 20.197639\n",
      "2022-02-10 19:18: Train Epoch 11: 40/158 Loss: 17.033096\n",
      "2022-02-10 19:20: Train Epoch 11: 60/158 Loss: 20.627188\n",
      "2022-02-10 19:21: Train Epoch 11: 80/158 Loss: 21.185131\n",
      "2022-02-10 19:22: Train Epoch 11: 100/158 Loss: 18.923132\n",
      "2022-02-10 19:24: Train Epoch 11: 120/158 Loss: 20.750072\n",
      "2022-02-10 19:25: Train Epoch 11: 140/158 Loss: 21.153481\n",
      "2022-02-10 19:26: **********Train Epoch 11: averaged Loss: 20.187816\n",
      "2022-02-10 19:27: **********Val Epoch 11: average Loss: 21.062637\n",
      "2022-02-10 19:27: *********************************Current best model saved!\n",
      "2022-02-10 19:27: Train Epoch 12: 0/158 Loss: 19.501440\n",
      "2022-02-10 19:28: Train Epoch 12: 20/158 Loss: 19.241158\n",
      "2022-02-10 19:29: Train Epoch 12: 40/158 Loss: 19.005964\n",
      "2022-02-10 19:31: Train Epoch 12: 60/158 Loss: 20.803905\n",
      "2022-02-10 19:32: Train Epoch 12: 80/158 Loss: 20.566582\n",
      "2022-02-10 19:33: Train Epoch 12: 100/158 Loss: 19.209837\n",
      "2022-02-10 19:35: Train Epoch 12: 120/158 Loss: 19.859344\n",
      "2022-02-10 19:36: Train Epoch 12: 140/158 Loss: 20.959047\n",
      "2022-02-10 19:37: **********Train Epoch 12: averaged Loss: 19.968570\n",
      "2022-02-10 19:38: **********Val Epoch 12: average Loss: 20.856815\n",
      "2022-02-10 19:38: *********************************Current best model saved!\n",
      "2022-02-10 19:38: Train Epoch 13: 0/158 Loss: 19.286598\n",
      "2022-02-10 19:39: Train Epoch 13: 20/158 Loss: 19.873573\n",
      "2022-02-10 19:41: Train Epoch 13: 40/158 Loss: 21.528589\n",
      "2022-02-10 19:42: Train Epoch 13: 60/158 Loss: 17.481033\n",
      "2022-02-10 19:43: Train Epoch 13: 80/158 Loss: 21.476519\n",
      "2022-02-10 19:45: Train Epoch 13: 100/158 Loss: 19.043274\n",
      "2022-02-10 19:46: Train Epoch 13: 120/158 Loss: 21.272722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 19:47: Train Epoch 13: 140/158 Loss: 20.809467\n",
      "2022-02-10 19:48: **********Train Epoch 13: averaged Loss: 19.691564\n",
      "2022-02-10 19:49: **********Val Epoch 13: average Loss: 21.213859\n",
      "2022-02-10 19:49: Train Epoch 14: 0/158 Loss: 19.771690\n",
      "2022-02-10 19:51: Train Epoch 14: 20/158 Loss: 19.351835\n",
      "2022-02-10 19:52: Train Epoch 14: 40/158 Loss: 19.413378\n",
      "2022-02-10 19:53: Train Epoch 14: 60/158 Loss: 20.714151\n",
      "2022-02-10 19:54: Train Epoch 14: 80/158 Loss: 20.464701\n",
      "2022-02-10 19:56: Train Epoch 14: 100/158 Loss: 20.458221\n",
      "2022-02-10 19:57: Train Epoch 14: 120/158 Loss: 19.932886\n",
      "2022-02-10 19:58: Train Epoch 14: 140/158 Loss: 18.826841\n",
      "2022-02-10 19:59: **********Train Epoch 14: averaged Loss: 19.595515\n",
      "2022-02-10 20:00: **********Val Epoch 14: average Loss: 20.435457\n",
      "2022-02-10 20:00: *********************************Current best model saved!\n",
      "2022-02-10 20:00: Train Epoch 15: 0/158 Loss: 20.843346\n",
      "2022-02-10 20:02: Train Epoch 15: 20/158 Loss: 18.235737\n",
      "2022-02-10 20:03: Train Epoch 15: 40/158 Loss: 17.279902\n",
      "2022-02-10 20:04: Train Epoch 15: 60/158 Loss: 20.357115\n",
      "2022-02-10 20:05: Train Epoch 15: 80/158 Loss: 19.530359\n",
      "2022-02-10 20:07: Train Epoch 15: 100/158 Loss: 18.333139\n",
      "2022-02-10 20:08: Train Epoch 15: 120/158 Loss: 18.993040\n",
      "2022-02-10 20:09: Train Epoch 15: 140/158 Loss: 21.606310\n",
      "2022-02-10 20:10: **********Train Epoch 15: averaged Loss: 19.650696\n",
      "2022-02-10 20:11: **********Val Epoch 15: average Loss: 21.294198\n",
      "2022-02-10 20:11: Train Epoch 16: 0/158 Loss: 18.197943\n",
      "2022-02-10 20:13: Train Epoch 16: 20/158 Loss: 19.407661\n",
      "2022-02-10 20:14: Train Epoch 16: 40/158 Loss: 19.870955\n",
      "2022-02-10 20:15: Train Epoch 16: 60/158 Loss: 19.573421\n",
      "2022-02-10 20:16: Train Epoch 16: 80/158 Loss: 20.252710\n",
      "2022-02-10 20:18: Train Epoch 16: 100/158 Loss: 20.670637\n",
      "2022-02-10 20:19: Train Epoch 16: 120/158 Loss: 18.908266\n",
      "2022-02-10 20:20: Train Epoch 16: 140/158 Loss: 21.418772\n",
      "2022-02-10 20:21: **********Train Epoch 16: averaged Loss: 19.341684\n",
      "2022-02-10 20:22: **********Val Epoch 16: average Loss: 20.845485\n",
      "2022-02-10 20:22: Train Epoch 17: 0/158 Loss: 21.427515\n",
      "2022-02-10 20:24: Train Epoch 17: 20/158 Loss: 19.307131\n",
      "2022-02-10 20:25: Train Epoch 17: 40/158 Loss: 20.906551\n",
      "2022-02-10 20:26: Train Epoch 17: 60/158 Loss: 19.086637\n",
      "2022-02-10 20:27: Train Epoch 17: 80/158 Loss: 20.664076\n",
      "2022-02-10 20:29: Train Epoch 17: 100/158 Loss: 19.362310\n",
      "2022-02-10 20:30: Train Epoch 17: 120/158 Loss: 18.369482\n",
      "2022-02-10 20:31: Train Epoch 17: 140/158 Loss: 20.371515\n",
      "2022-02-10 20:32: **********Train Epoch 17: averaged Loss: 19.621468\n",
      "2022-02-10 20:33: **********Val Epoch 17: average Loss: 20.540702\n",
      "2022-02-10 20:33: Train Epoch 18: 0/158 Loss: 18.364901\n",
      "2022-02-10 20:35: Train Epoch 18: 20/158 Loss: 18.472860\n",
      "2022-02-10 20:36: Train Epoch 18: 40/158 Loss: 18.941389\n",
      "2022-02-10 20:37: Train Epoch 18: 60/158 Loss: 21.086020\n",
      "2022-02-10 20:38: Train Epoch 18: 80/158 Loss: 18.758831\n",
      "2022-02-10 20:40: Train Epoch 18: 100/158 Loss: 18.525700\n",
      "2022-02-10 20:41: Train Epoch 18: 120/158 Loss: 17.763155\n",
      "2022-02-10 20:42: Train Epoch 18: 140/158 Loss: 18.519783\n",
      "2022-02-10 20:43: **********Train Epoch 18: averaged Loss: 19.177646\n",
      "2022-02-10 20:44: **********Val Epoch 18: average Loss: 20.365619\n",
      "2022-02-10 20:44: *********************************Current best model saved!\n",
      "2022-02-10 20:44: Train Epoch 19: 0/158 Loss: 19.557894\n",
      "2022-02-10 20:46: Train Epoch 19: 20/158 Loss: 20.540617\n",
      "2022-02-10 20:47: Train Epoch 19: 40/158 Loss: 19.762165\n",
      "2022-02-10 20:48: Train Epoch 19: 60/158 Loss: 18.676460\n",
      "2022-02-10 20:51: Train Epoch 19: 100/158 Loss: 16.888384\n",
      "2022-02-10 20:52: Train Epoch 19: 120/158 Loss: 18.560320\n",
      "2022-02-10 20:53: Train Epoch 19: 140/158 Loss: 18.874178\n",
      "2022-02-10 20:54: **********Train Epoch 19: averaged Loss: 19.077364\n",
      "2022-02-10 20:55: **********Val Epoch 19: average Loss: 20.407096\n",
      "2022-02-10 20:55: Train Epoch 20: 0/158 Loss: 19.756575\n",
      "2022-02-10 20:57: Train Epoch 20: 20/158 Loss: 19.443645\n",
      "2022-02-10 20:58: Train Epoch 20: 40/158 Loss: 18.489691\n",
      "2022-02-10 20:59: Train Epoch 20: 60/158 Loss: 18.397799\n",
      "2022-02-10 21:01: Train Epoch 20: 80/158 Loss: 18.935337\n",
      "2022-02-10 21:02: Train Epoch 20: 100/158 Loss: 18.931540\n",
      "2022-02-10 21:03: Train Epoch 20: 120/158 Loss: 20.235304\n",
      "2022-02-10 21:04: Train Epoch 20: 140/158 Loss: 18.769089\n",
      "2022-02-10 21:05: **********Train Epoch 20: averaged Loss: 19.173563\n",
      "2022-02-10 21:06: **********Val Epoch 20: average Loss: 20.243638\n",
      "2022-02-10 21:06: *********************************Current best model saved!\n",
      "2022-02-10 21:06: Train Epoch 21: 0/158 Loss: 17.947975\n",
      "2022-02-10 21:08: Train Epoch 21: 20/158 Loss: 19.667133\n",
      "2022-02-10 21:09: Train Epoch 21: 40/158 Loss: 17.818056\n",
      "2022-02-10 21:10: Train Epoch 21: 60/158 Loss: 19.214537\n",
      "2022-02-10 21:12: Train Epoch 21: 80/158 Loss: 19.067198\n",
      "2022-02-10 21:13: Train Epoch 21: 100/158 Loss: 19.782572\n",
      "2022-02-10 21:14: Train Epoch 21: 120/158 Loss: 18.815104\n",
      "2022-02-10 21:15: Train Epoch 21: 140/158 Loss: 18.001747\n",
      "2022-02-10 21:17: **********Train Epoch 21: averaged Loss: 18.873913\n",
      "2022-02-10 21:18: **********Val Epoch 21: average Loss: 21.172592\n",
      "2022-02-10 21:18: Train Epoch 22: 0/158 Loss: 18.340532\n",
      "2022-02-10 21:19: Train Epoch 22: 20/158 Loss: 19.838810\n",
      "2022-02-10 21:20: Train Epoch 22: 40/158 Loss: 17.819019\n",
      "2022-02-10 21:21: Train Epoch 22: 60/158 Loss: 19.533426\n",
      "2022-02-10 21:23: Train Epoch 22: 80/158 Loss: 18.005630\n",
      "2022-02-10 21:24: Train Epoch 22: 100/158 Loss: 19.714182\n",
      "2022-02-10 21:25: Train Epoch 22: 120/158 Loss: 19.781843\n",
      "2022-02-10 21:27: Train Epoch 22: 140/158 Loss: 19.421888\n",
      "2022-02-10 21:28: **********Train Epoch 22: averaged Loss: 18.852628\n",
      "2022-02-10 21:29: **********Val Epoch 22: average Loss: 20.178347\n",
      "2022-02-10 21:29: *********************************Current best model saved!\n",
      "2022-02-10 21:29: Train Epoch 23: 0/158 Loss: 17.932470\n",
      "2022-02-10 21:30: Train Epoch 23: 20/158 Loss: 18.712547\n",
      "2022-02-10 21:31: Train Epoch 23: 40/158 Loss: 18.428946\n",
      "2022-02-10 21:33: Train Epoch 23: 60/158 Loss: 18.576862\n",
      "2022-02-10 21:34: Train Epoch 23: 80/158 Loss: 19.730446\n",
      "2022-02-10 21:35: Train Epoch 23: 100/158 Loss: 17.977844\n",
      "2022-02-10 21:37: Train Epoch 23: 120/158 Loss: 17.860373\n",
      "2022-02-10 21:38: Train Epoch 23: 140/158 Loss: 20.091665\n",
      "2022-02-10 21:39: **********Train Epoch 23: averaged Loss: 19.238271\n",
      "2022-02-10 21:40: **********Val Epoch 23: average Loss: 20.205633\n",
      "2022-02-10 21:40: Train Epoch 24: 0/158 Loss: 19.195585\n",
      "2022-02-10 21:41: Train Epoch 24: 20/158 Loss: 20.017162\n",
      "2022-02-10 21:43: Train Epoch 24: 40/158 Loss: 18.983795\n",
      "2022-02-10 21:44: Train Epoch 24: 60/158 Loss: 18.100275\n",
      "2022-02-10 21:45: Train Epoch 24: 80/158 Loss: 20.812140\n",
      "2022-02-10 21:47: Train Epoch 24: 100/158 Loss: 20.108093\n",
      "2022-02-10 21:48: Train Epoch 24: 120/158 Loss: 17.686266\n",
      "2022-02-10 21:49: Train Epoch 24: 140/158 Loss: 18.964182\n",
      "2022-02-10 21:50: **********Train Epoch 24: averaged Loss: 19.000140\n",
      "2022-02-10 21:51: **********Val Epoch 24: average Loss: 20.223449\n",
      "2022-02-10 21:51: Train Epoch 25: 0/158 Loss: 19.455914\n",
      "2022-02-10 21:52: Train Epoch 25: 20/158 Loss: 19.003153\n",
      "2022-02-10 21:54: Train Epoch 25: 40/158 Loss: 17.974447\n",
      "2022-02-10 21:55: Train Epoch 25: 60/158 Loss: 19.267250\n",
      "2022-02-10 21:56: Train Epoch 25: 80/158 Loss: 18.356258\n",
      "2022-02-10 21:58: Train Epoch 25: 100/158 Loss: 18.001049\n",
      "2022-02-10 21:59: Train Epoch 25: 120/158 Loss: 17.357357\n",
      "2022-02-10 22:00: Train Epoch 25: 140/158 Loss: 19.105167\n",
      "2022-02-10 22:02: **********Train Epoch 25: averaged Loss: 18.909315\n",
      "2022-02-10 22:03: **********Val Epoch 25: average Loss: 22.335994\n",
      "2022-02-10 22:03: Train Epoch 26: 0/158 Loss: 20.298216\n",
      "2022-02-10 22:04: Train Epoch 26: 20/158 Loss: 18.344147\n",
      "2022-02-10 22:06: Train Epoch 26: 40/158 Loss: 19.060032\n",
      "2022-02-10 22:07: Train Epoch 26: 60/158 Loss: 18.520761\n",
      "2022-02-10 22:08: Train Epoch 26: 80/158 Loss: 20.786894\n",
      "2022-02-10 22:09: Train Epoch 26: 100/158 Loss: 17.262150\n",
      "2022-02-10 22:11: Train Epoch 26: 120/158 Loss: 17.344145\n",
      "2022-02-10 22:12: Train Epoch 26: 140/158 Loss: 19.336815\n",
      "2022-02-10 22:13: **********Train Epoch 26: averaged Loss: 18.705086\n",
      "2022-02-10 22:14: **********Val Epoch 26: average Loss: 20.164376\n",
      "2022-02-10 22:14: *********************************Current best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 22:14: Train Epoch 27: 0/158 Loss: 18.403982\n",
      "2022-02-10 22:16: Train Epoch 27: 20/158 Loss: 17.235260\n",
      "2022-02-10 22:17: Train Epoch 27: 40/158 Loss: 19.226721\n",
      "2022-02-10 22:18: Train Epoch 27: 60/158 Loss: 21.107458\n",
      "2022-02-10 22:19: Train Epoch 27: 80/158 Loss: 18.809050\n",
      "2022-02-10 22:21: Train Epoch 27: 100/158 Loss: 23.186954\n",
      "2022-02-10 22:22: Train Epoch 27: 120/158 Loss: 28.334248\n",
      "2022-02-10 22:23: Train Epoch 27: 140/158 Loss: 27.366379\n",
      "2022-02-10 22:24: **********Train Epoch 27: averaged Loss: 22.360649\n",
      "2022-02-10 22:25: **********Val Epoch 27: average Loss: 29.309803\n",
      "2022-02-10 22:25: Train Epoch 28: 0/158 Loss: 30.227592\n",
      "2022-02-10 22:27: Train Epoch 28: 20/158 Loss: 21.539219\n",
      "2022-02-10 22:28: Train Epoch 28: 40/158 Loss: 20.637924\n",
      "2022-02-10 22:29: Train Epoch 28: 60/158 Loss: 19.880579\n",
      "2022-02-10 22:30: Train Epoch 28: 80/158 Loss: 20.298031\n",
      "2022-02-10 22:32: Train Epoch 28: 100/158 Loss: 17.868605\n",
      "2022-02-10 22:33: Train Epoch 28: 120/158 Loss: 20.308828\n",
      "2022-02-10 22:34: Train Epoch 28: 140/158 Loss: 20.089237\n",
      "2022-02-10 22:35: **********Train Epoch 28: averaged Loss: 20.637669\n",
      "2022-02-10 22:36: **********Val Epoch 28: average Loss: 20.706892\n",
      "2022-02-10 22:36: Train Epoch 29: 0/158 Loss: 20.626459\n",
      "2022-02-10 22:38: Train Epoch 29: 20/158 Loss: 18.760685\n",
      "2022-02-10 22:39: Train Epoch 29: 40/158 Loss: 19.732981\n",
      "2022-02-10 22:40: Train Epoch 29: 60/158 Loss: 17.735575\n",
      "2022-02-10 22:42: Train Epoch 29: 80/158 Loss: 18.670584\n",
      "2022-02-10 22:43: Train Epoch 29: 100/158 Loss: 19.445831\n",
      "2022-02-10 22:44: Train Epoch 29: 120/158 Loss: 18.275734\n",
      "2022-02-10 22:46: Train Epoch 29: 140/158 Loss: 18.881329\n",
      "2022-02-10 22:47: **********Train Epoch 29: averaged Loss: 19.192572\n",
      "2022-02-10 22:48: **********Val Epoch 29: average Loss: 20.333777\n",
      "2022-02-10 22:48: Train Epoch 30: 0/158 Loss: 19.177555\n",
      "2022-02-10 22:49: Train Epoch 30: 20/158 Loss: 19.105495\n",
      "2022-02-10 22:51: Train Epoch 30: 40/158 Loss: 19.836494\n",
      "2022-02-10 22:52: Train Epoch 30: 60/158 Loss: 18.947834\n",
      "2022-02-10 22:53: Train Epoch 30: 80/158 Loss: 18.299139\n",
      "2022-02-10 22:55: Train Epoch 30: 100/158 Loss: 16.704340\n",
      "2022-02-10 22:56: Train Epoch 30: 120/158 Loss: 19.271770\n",
      "2022-02-10 22:57: Train Epoch 30: 140/158 Loss: 18.914299\n",
      "2022-02-10 22:59: **********Train Epoch 30: averaged Loss: 18.766352\n",
      "2022-02-10 23:00: **********Val Epoch 30: average Loss: 20.552422\n",
      "2022-02-10 23:00: Train Epoch 31: 0/158 Loss: 18.605167\n",
      "2022-02-10 23:01: Train Epoch 31: 20/158 Loss: 19.584431\n",
      "2022-02-10 23:02: Train Epoch 31: 40/158 Loss: 18.994139\n",
      "2022-02-10 23:03: Train Epoch 31: 60/158 Loss: 18.346210\n",
      "2022-02-10 23:05: Train Epoch 31: 80/158 Loss: 18.453472\n",
      "2022-02-10 23:06: Train Epoch 31: 100/158 Loss: 18.198576\n",
      "2022-02-10 23:07: Train Epoch 31: 120/158 Loss: 18.627415\n",
      "2022-02-10 23:09: Train Epoch 31: 140/158 Loss: 20.184986\n",
      "2022-02-10 23:10: **********Train Epoch 31: averaged Loss: 18.903468\n",
      "2022-02-10 23:11: **********Val Epoch 31: average Loss: 20.015771\n",
      "2022-02-10 23:11: *********************************Current best model saved!\n",
      "2022-02-10 23:11: Train Epoch 32: 0/158 Loss: 18.519384\n",
      "2022-02-10 23:12: Train Epoch 32: 20/158 Loss: 20.205662\n",
      "2022-02-10 23:13: Train Epoch 32: 40/158 Loss: 23.005299\n",
      "2022-02-10 23:14: Train Epoch 32: 60/158 Loss: 20.656206\n",
      "2022-02-10 23:16: Train Epoch 32: 80/158 Loss: 19.499292\n",
      "2022-02-10 23:17: Train Epoch 32: 100/158 Loss: 18.150076\n",
      "2022-02-10 23:18: Train Epoch 32: 120/158 Loss: 19.333223\n",
      "2022-02-10 23:20: Train Epoch 32: 140/158 Loss: 18.317270\n",
      "2022-02-10 23:21: **********Train Epoch 32: averaged Loss: 19.467750\n",
      "2022-02-10 23:22: **********Val Epoch 32: average Loss: 20.003383\n",
      "2022-02-10 23:22: *********************************Current best model saved!\n",
      "2022-02-10 23:22: Train Epoch 33: 0/158 Loss: 17.108315\n",
      "2022-02-10 23:23: Train Epoch 33: 20/158 Loss: 17.627068\n",
      "2022-02-10 23:25: Train Epoch 33: 40/158 Loss: 19.962385\n",
      "2022-02-10 23:26: Train Epoch 33: 60/158 Loss: 19.154135\n",
      "2022-02-10 23:28: Train Epoch 33: 80/158 Loss: 19.768772\n",
      "2022-02-10 23:29: Train Epoch 33: 100/158 Loss: 18.669455\n",
      "2022-02-10 23:31: Train Epoch 33: 120/158 Loss: 18.596981\n",
      "2022-02-10 23:32: Train Epoch 33: 140/158 Loss: 18.644169\n",
      "2022-02-10 23:34: **********Train Epoch 33: averaged Loss: 18.661734\n",
      "2022-02-10 23:35: **********Val Epoch 33: average Loss: 20.391352\n",
      "2022-02-10 23:35: Train Epoch 34: 0/158 Loss: 20.150049\n",
      "2022-02-10 23:36: Train Epoch 34: 20/158 Loss: 19.679073\n",
      "2022-02-10 23:38: Train Epoch 34: 40/158 Loss: 19.710302\n",
      "2022-02-10 23:39: Train Epoch 34: 60/158 Loss: 18.400291\n",
      "2022-02-10 23:41: Train Epoch 34: 80/158 Loss: 19.370871\n",
      "2022-02-10 23:42: Train Epoch 34: 100/158 Loss: 17.858286\n",
      "2022-02-10 23:44: Train Epoch 34: 120/158 Loss: 18.851406\n",
      "2022-02-10 23:45: Train Epoch 34: 140/158 Loss: 18.279131\n",
      "2022-02-10 23:46: **********Train Epoch 34: averaged Loss: 18.413840\n",
      "2022-02-10 23:47: **********Val Epoch 34: average Loss: 20.016253\n",
      "2022-02-10 23:47: Train Epoch 35: 0/158 Loss: 19.690750\n",
      "2022-02-10 23:49: Train Epoch 35: 20/158 Loss: 17.593430\n",
      "2022-02-10 23:50: Train Epoch 35: 40/158 Loss: 18.394812\n",
      "2022-02-10 23:52: Train Epoch 35: 60/158 Loss: 18.270947\n",
      "2022-02-10 23:53: Train Epoch 35: 80/158 Loss: 18.176216\n",
      "2022-02-10 23:55: Train Epoch 35: 100/158 Loss: 20.469034\n",
      "2022-02-10 23:56: Train Epoch 35: 120/158 Loss: 17.734486\n",
      "2022-02-10 23:58: Train Epoch 35: 140/158 Loss: 20.675734\n",
      "2022-02-10 23:59: **********Train Epoch 35: averaged Loss: 18.459856\n",
      "2022-02-11 00:00: **********Val Epoch 35: average Loss: 20.151963\n",
      "2022-02-11 00:00: Train Epoch 36: 0/158 Loss: 16.740746\n",
      "2022-02-11 00:02: Train Epoch 36: 20/158 Loss: 17.967585\n",
      "2022-02-11 00:03: Train Epoch 36: 40/158 Loss: 20.959599\n",
      "2022-02-11 00:05: Train Epoch 36: 60/158 Loss: 27.821934\n",
      "2022-02-11 00:06: Train Epoch 36: 80/158 Loss: 21.413177\n",
      "2022-02-11 00:08: Train Epoch 36: 100/158 Loss: 19.701586\n",
      "2022-02-11 00:09: Train Epoch 36: 120/158 Loss: 19.617975\n",
      "2022-02-11 00:11: Train Epoch 36: 140/158 Loss: 18.981134\n",
      "2022-02-11 00:12: **********Train Epoch 36: averaged Loss: 20.187587\n",
      "2022-02-11 00:13: **********Val Epoch 36: average Loss: 20.292272\n",
      "2022-02-11 00:13: Train Epoch 37: 0/158 Loss: 18.717960\n",
      "2022-02-11 00:14: Train Epoch 37: 20/158 Loss: 19.575052\n",
      "2022-02-11 00:15: Train Epoch 37: 40/158 Loss: 19.731075\n",
      "2022-02-11 00:16: Train Epoch 37: 60/158 Loss: 17.992325\n",
      "2022-02-11 00:18: Train Epoch 37: 80/158 Loss: 73.666069\n",
      "2022-02-11 00:19: Train Epoch 37: 100/158 Loss: 35.005322\n",
      "2022-02-11 00:20: Train Epoch 37: 120/158 Loss: 30.956303\n",
      "2022-02-11 00:22: Train Epoch 37: 140/158 Loss: 43.021523\n",
      "2022-02-11 00:23: **********Train Epoch 37: averaged Loss: 34.253591\n",
      "2022-02-11 00:24: **********Val Epoch 37: average Loss: 32.374321\n",
      "2022-02-11 00:24: Train Epoch 38: 0/158 Loss: 30.500849\n",
      "2022-02-11 00:25: Train Epoch 38: 20/158 Loss: 30.018913\n",
      "2022-02-11 00:26: Train Epoch 38: 40/158 Loss: 28.992641\n",
      "2022-02-11 00:27: Train Epoch 38: 60/158 Loss: 31.966370\n",
      "2022-02-11 00:29: Train Epoch 38: 80/158 Loss: 28.513453\n",
      "2022-02-11 00:30: Train Epoch 38: 100/158 Loss: 25.987612\n",
      "2022-02-11 00:31: Train Epoch 38: 120/158 Loss: 27.279135\n",
      "2022-02-11 00:33: Train Epoch 38: 140/158 Loss: 25.550676\n",
      "2022-02-11 00:34: **********Train Epoch 38: averaged Loss: 28.219722\n",
      "2022-02-11 00:35: **********Val Epoch 38: average Loss: 27.092275\n",
      "2022-02-11 00:35: Train Epoch 39: 0/158 Loss: 26.567909\n",
      "2022-02-11 00:36: Train Epoch 39: 20/158 Loss: 31.266546\n",
      "2022-02-11 00:37: Train Epoch 39: 40/158 Loss: 29.455914\n",
      "2022-02-11 00:38: Train Epoch 39: 60/158 Loss: 27.383671\n",
      "2022-02-11 00:40: Train Epoch 39: 80/158 Loss: 26.162371\n",
      "2022-02-11 00:41: Train Epoch 39: 100/158 Loss: 25.075167\n",
      "2022-02-11 00:42: Train Epoch 39: 120/158 Loss: 77.857208\n",
      "2022-02-11 00:44: Train Epoch 39: 140/158 Loss: 31.649960\n",
      "2022-02-11 00:45: **********Train Epoch 39: averaged Loss: 33.992226\n",
      "2022-02-11 00:46: **********Val Epoch 39: average Loss: 31.325138\n",
      "2022-02-11 00:46: Train Epoch 40: 0/158 Loss: 29.612854\n",
      "2022-02-11 00:47: Train Epoch 40: 20/158 Loss: 26.019102\n",
      "2022-02-11 00:48: Train Epoch 40: 40/158 Loss: 31.280205\n",
      "2022-02-11 00:49: Train Epoch 40: 60/158 Loss: 29.182116\n",
      "2022-02-11 00:51: Train Epoch 40: 80/158 Loss: 25.807669\n",
      "2022-02-11 00:52: Train Epoch 40: 100/158 Loss: 24.779802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 00:53: Train Epoch 40: 120/158 Loss: 25.119024\n",
      "2022-02-11 00:55: Train Epoch 40: 140/158 Loss: 28.762632\n",
      "2022-02-11 00:56: **********Train Epoch 40: averaged Loss: 26.556906\n",
      "2022-02-11 00:57: **********Val Epoch 40: average Loss: 29.552981\n",
      "2022-02-11 00:57: Train Epoch 41: 0/158 Loss: 29.481506\n",
      "2022-02-11 00:58: Train Epoch 41: 20/158 Loss: 34.286579\n",
      "2022-02-11 00:59: Train Epoch 41: 40/158 Loss: 30.762569\n",
      "2022-02-11 01:00: Train Epoch 41: 60/158 Loss: 52.150764\n",
      "2022-02-11 01:02: Train Epoch 41: 80/158 Loss: 30.732565\n",
      "2022-02-11 01:03: Train Epoch 41: 100/158 Loss: 26.971004\n",
      "2022-02-11 01:04: Train Epoch 41: 120/158 Loss: 29.567608\n",
      "2022-02-11 01:06: Train Epoch 41: 140/158 Loss: 29.813307\n",
      "2022-02-11 01:07: **********Train Epoch 41: averaged Loss: 32.829339\n",
      "2022-02-11 01:08: **********Val Epoch 41: average Loss: 28.849851\n",
      "2022-02-11 01:08: Train Epoch 42: 0/158 Loss: 28.395657\n",
      "2022-02-11 01:09: Train Epoch 42: 20/158 Loss: 25.838446\n",
      "2022-02-11 01:10: Train Epoch 42: 40/158 Loss: 26.984453\n",
      "2022-02-11 01:11: Train Epoch 42: 60/158 Loss: 25.494385\n",
      "2022-02-11 01:13: Train Epoch 42: 80/158 Loss: 29.685669\n",
      "2022-02-11 01:14: Train Epoch 42: 100/158 Loss: 26.444139\n",
      "2022-02-11 01:15: Train Epoch 42: 120/158 Loss: 26.614412\n",
      "2022-02-11 01:17: Train Epoch 42: 140/158 Loss: 25.733147\n",
      "2022-02-11 01:18: **********Train Epoch 42: averaged Loss: 26.721968\n",
      "2022-02-11 01:19: **********Val Epoch 42: average Loss: 27.018727\n",
      "2022-02-11 01:19: Train Epoch 43: 0/158 Loss: 23.081314\n",
      "2022-02-11 01:20: Train Epoch 43: 20/158 Loss: 25.367016\n",
      "2022-02-11 01:21: Train Epoch 43: 40/158 Loss: 28.105591\n",
      "2022-02-11 01:22: Train Epoch 43: 60/158 Loss: 25.233273\n",
      "2022-02-11 01:24: Train Epoch 43: 80/158 Loss: 24.991858\n",
      "2022-02-11 01:25: Train Epoch 43: 100/158 Loss: 23.548960\n",
      "2022-02-11 01:26: Train Epoch 43: 120/158 Loss: 26.216225\n",
      "2022-02-11 01:28: Train Epoch 43: 140/158 Loss: 25.102856\n",
      "2022-02-11 01:29: **********Train Epoch 43: averaged Loss: 24.592443\n",
      "2022-02-11 01:30: **********Val Epoch 43: average Loss: 25.707350\n",
      "2022-02-11 01:30: Train Epoch 44: 0/158 Loss: 23.804569\n",
      "2022-02-11 01:31: Train Epoch 44: 20/158 Loss: 23.448528\n",
      "2022-02-11 01:32: Train Epoch 44: 40/158 Loss: 24.322922\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "file_dir = os.path.dirname(os.path.dirname(os.path.abspath(\"/home/rodr/code/mobility-baselines/STG-NCDE-main\")))\n",
    "print(file_dir)\n",
    "sys.path.append(file_dir)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import configparser\n",
    "import time\n",
    "\n",
    "from model.BasicTrainer_cde import Trainer\n",
    "from lib.TrainInits import init_seed\n",
    "from lib.dataloader import get_dataloader_cde\n",
    "from lib.TrainInits import print_model_parameters\n",
    "import os\n",
    "from os.path import join\n",
    "from model.Make_model import make_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#*************************************************************************#\n",
    "Mode = 'train'\n",
    "DEBUG = 'False'\n",
    "DATASET = 'PEMSD4'      #PEMSD4 or PEMSD8\n",
    "MODEL = 'GCDE'\n",
    "\n",
    "#get configuration\n",
    "config_file = 'model/{}_{}.conf'.format(DATASET, MODEL)\n",
    "#print('Read configuration file: %s' % (config_file))\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "from lib.metrics import MAE_torch\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "    return loss\n",
    "\n",
    "# python Run_cde.py --dataset='PEMSD4' --model='GCDE' --model_type='type1' --embed_dim=10 --hid_dim=64 --hid_hid_dim=64 --num_layers=2 --lr_init=0.001 --weight_decay=1e-3 --epochs=200 --tensorboard --comment=\"\" --device=0\n",
    "\n",
    "#parser\n",
    "args = argparse.ArgumentParser(description='arguments')\n",
    "args.add_argument('--dataset', default=DATASET, type=str)\n",
    "args.add_argument('--mode', default=Mode, type=str)\n",
    "args.add_argument('--device', default=0, type=int, help='indices of GPUs')\n",
    "args.add_argument('--debug', default=DEBUG, type=eval)\n",
    "args.add_argument('--model', default=MODEL, type=str)\n",
    "args.add_argument('--cuda', default=True, type=bool)\n",
    "args.add_argument('--comment', default='', type=str)\n",
    "args.add_argument(\"-f\", type=str, default=\"\")\n",
    "\n",
    "\n",
    "#data\n",
    "args.add_argument('--val_ratio', default=config['data']['val_ratio'], type=float)\n",
    "args.add_argument('--test_ratio', default=config['data']['test_ratio'], type=float)\n",
    "args.add_argument('--lag', default=config['data']['lag'], type=int)\n",
    "args.add_argument('--horizon', default=config['data']['horizon'], type=int)\n",
    "args.add_argument('--num_nodes', default=config['data']['num_nodes'], type=int)\n",
    "args.add_argument('--tod', default=config['data']['tod'], type=eval)\n",
    "args.add_argument('--normalizer', default=config['data']['normalizer'], type=str)\n",
    "args.add_argument('--column_wise', default=config['data']['column_wise'], type=eval)\n",
    "args.add_argument('--default_graph', default=config['data']['default_graph'], type=eval)\n",
    "#model\n",
    "args.add_argument('--model_type', default=config['model']['type'], type=str)\n",
    "args.add_argument('--g_type', default=config['model']['g_type'], type=str)\n",
    "args.add_argument('--input_dim', default=config['model']['input_dim'], type=int)\n",
    "args.add_argument('--output_dim', default=config['model']['output_dim'], type=int)\n",
    "args.add_argument('--embed_dim', default=config['model']['embed_dim'], type=int)\n",
    "args.add_argument('--hid_dim', default=config['model']['hid_dim'], type=int)\n",
    "args.add_argument('--hid_hid_dim', default=config['model']['hid_hid_dim'], type=int)\n",
    "args.add_argument('--num_layers', default=config['model']['num_layers'], type=int)\n",
    "args.add_argument('--cheb_k', default=config['model']['cheb_order'], type=int)\n",
    "args.add_argument('--solver', default='rk4', type=str)\n",
    "\n",
    "#train\n",
    "args.add_argument('--loss_func', default=config['train']['loss_func'], type=str)\n",
    "args.add_argument('--seed', default=config['train']['seed'], type=int)\n",
    "args.add_argument('--batch_size', default=config['train']['batch_size'], type=int)\n",
    "args.add_argument('--epochs', default=config['train']['epochs'], type=int)\n",
    "args.add_argument('--lr_init', default=config['train']['lr_init'], type=float)\n",
    "args.add_argument('--weight_decay', default=config['train']['weight_decay'], type=eval)\n",
    "args.add_argument('--lr_decay', default=config['train']['lr_decay'], type=eval)\n",
    "args.add_argument('--lr_decay_rate', default=config['train']['lr_decay_rate'], type=float)\n",
    "args.add_argument('--lr_decay_step', default=config['train']['lr_decay_step'], type=str)\n",
    "args.add_argument('--early_stop', default=config['train']['early_stop'], type=eval)\n",
    "args.add_argument('--early_stop_patience', default=config['train']['early_stop_patience'], type=int)\n",
    "args.add_argument('--grad_norm', default=config['train']['grad_norm'], type=eval)\n",
    "args.add_argument('--max_grad_norm', default=config['train']['max_grad_norm'], type=int)\n",
    "args.add_argument('--teacher_forcing', default=False, type=bool)\n",
    "#args.add_argument('--tf_decay_steps', default=2000, type=int, help='teacher forcing decay steps')\n",
    "args.add_argument('--real_value', default=config['train']['real_value'], type=eval, help = 'use real value for loss calculation')\n",
    "\n",
    "args.add_argument('--missing_test', default=False, type=bool)\n",
    "args.add_argument('--missing_rate', default=0.1, type=float)\n",
    "\n",
    "#test\n",
    "args.add_argument('--mae_thresh', default=config['test']['mae_thresh'], type=eval)\n",
    "args.add_argument('--mape_thresh', default=config['test']['mape_thresh'], type=float)\n",
    "args.add_argument('--model_path', default='', type=str)\n",
    "#log\n",
    "args.add_argument('--log_dir', default='../runs', type=str)\n",
    "args.add_argument('--log_step', default=config['log']['log_step'], type=int)\n",
    "args.add_argument('--plot', default=config['log']['plot'], type=eval)\n",
    "args.add_argument('--tensorboard',action='store_true',help='tensorboard')\n",
    "\n",
    "args = args.parse_args()\n",
    "init_seed(args.seed)\n",
    "\n",
    "GPU_NUM = args.device\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "print(args)\n",
    "\n",
    "#config log path\n",
    "save_name = time.strftime(\"%m-%d-%Hh%Mm\")+args.comment+\"_\"+ args.dataset+\"_\"+ args.model+\"_\"+ args.model_type+\"_\"+\"embed{\"+str(args.embed_dim)+\"}\"+\"hid{\"+str(args.hid_dim)+\"}\"+\"hidhid{\"+str(args.hid_hid_dim)+\"}\"+\"lyrs{\"+str(args.num_layers)+\"}\"+\"lr{\"+str(args.lr_init)+\"}\"+\"wd{\"+str(args.weight_decay)+\"}\"\n",
    "path = '../runs'\n",
    "\n",
    "log_dir = join(path, args.dataset, save_name)\n",
    "args.log_dir = log_dir\n",
    "if (os.path.exists(args.log_dir)):\n",
    "        print('has model save path')\n",
    "else:\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "if args.tensorboard:\n",
    "    w : SummaryWriter = SummaryWriter(args.log_dir)\n",
    "else:\n",
    "    w = None\n",
    "\n",
    "#init model\n",
    "if args.model_type=='type1':\n",
    "    model, vector_field_f, vector_field_g = make_model(args)\n",
    "elif args.model_type=='type1_temporal':\n",
    "    model, vector_field_f = make_model(args)\n",
    "elif args.model_type=='type1_spatial':\n",
    "    model, vector_field_g = make_model(args)\n",
    "else:\n",
    "    raise ValueError(\"Check args.model_type\")\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "if args.model_type=='type1_temporal':\n",
    "    vector_field_f = vector_field_f.to(args.device)\n",
    "    vector_field_g = None\n",
    "elif args.model_type=='type1_spatial':\n",
    "    vector_field_f = None\n",
    "    vector_field_g = vector_field_g.to(args.device)\n",
    "else:\n",
    "    vector_field_f = vector_field_f.to(args.device)\n",
    "    vector_field_g = vector_field_g.to(args.device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    else:\n",
    "        nn.init.uniform_(p)\n",
    "print_model_parameters(model, only_num=False)\n",
    "\n",
    "#load dataset\n",
    "train_loader, val_loader, test_loader, scaler, times = get_dataloader_cde(args,\n",
    "                                                               normalizer=args.normalizer,\n",
    "                                                               tod=args.tod, dow=False,\n",
    "                                                               weather=False, single=False)\n",
    "\n",
    "#init loss function, optimizer\n",
    "if args.loss_func == 'mask_mae':\n",
    "    loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "elif args.loss_func == 'mae':\n",
    "    loss = torch.nn.L1Loss().to(args.device)\n",
    "elif args.loss_func == 'mse':\n",
    "    loss = torch.nn.MSELoss().to(args.device)\n",
    "elif args.loss_func == 'huber_loss':\n",
    "    loss = torch.nn.HuberLoss(delta=1.0).to(args.device)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=args.lr_init,\n",
    "                             weight_decay=args.weight_decay)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = None\n",
    "if args.lr_decay:\n",
    "    print('Applying learning rate decay.')\n",
    "    lr_decay_steps = [int(i) for i in list(args.lr_decay_step.split(','))]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=lr_decay_steps,\n",
    "                                                        gamma=args.lr_decay_rate)\n",
    "\n",
    "#start training\n",
    "trainer = Trainer(model, vector_field_f, vector_field_g, loss, optimizer, train_loader, val_loader, test_loader, scaler,\n",
    "                  args, lr_scheduler, args.device, times,\n",
    "                  w)\n",
    "if args.mode == 'train':\n",
    "    trainer.train()\n",
    "elif args.mode == 'test':\n",
    "    model.load_state_dict(torch.load('./pre-trained/{}.pth'.format(args.dataset)))\n",
    "    print(\"Load saved model\")\n",
    "    trainer.test(model, trainer.args, test_loader, scaler, trainer.logger, times)\n",
    "else:\n",
    "    raise ValueError\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
